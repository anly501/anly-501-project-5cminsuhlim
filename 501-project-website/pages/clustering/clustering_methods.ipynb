{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "pdf-engine: lualatex\n",
        "format:\n",
        "  html:\n",
        "    theme: yeti\n",
        "    toc: true\n",
        "    code-fold: true\n",
        "    toc-title: Contents\n",
        "  pdf:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    fig-pos: t\n",
        "execute:\n",
        "  echo: true\n",
        "---"
      ],
      "id": "b42ff4c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clustering\n",
        "\n",
        "## Methods\n",
        "\n",
        "For our analysis below, we will be using a combined data set containing occupations, employment rates, gender proportions, and annual wages from the Bureau of Labor Statistics. We will perform three clustering methods: K-Means, DBSCAN, and Hierarchical (Agglomerative).\n",
        "\n",
        "### Imports\n"
      ],
      "id": "5a6224ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "6663394b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Selection\n"
      ],
      "id": "3f5c774f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv('../../../data/01-modified-data/occupations_detailed_(employment_by_sex_and_wage).csv')\n",
        "\n",
        "## drop unneeded column created from read_csv\n",
        "df = df.iloc[:, 1:]"
      ],
      "id": "2f4f5cd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## reorder columns\n",
        "df = df[['TOT_EMP', 'A_MEAN', 'Women (%)', 'Men (%)', 'Target', 'Target_Num', 'OCC_TITLE']]\n",
        "\n",
        "## rename columns\n",
        "df = df.rename(columns={'TOT_EMP':'Total Employment', 'A_MEAN':'Mean Annual Wage'})"
      ],
      "id": "c0c53271",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Selection\n",
        "\n",
        "As we can see, we have a heavily imbalanced data set.\n"
      ],
      "id": "78382ab9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['Target'].value_counts(ascending=True)"
      ],
      "id": "f48f9746",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We perform some pre-processing to ensure the best clustering results. First, we isolate the features in our data set, which are Total Employment, Average Annual Wage, % of Women in the occupation, and % of Men in the occupation.\n"
      ],
      "id": "6ba61a15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = df.iloc[:, 0:4]"
      ],
      "id": "c544f454",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalization\n",
        "\n",
        "Since our predictors (X) consist of values that are outside the ideal range of [0, 1], we first normalize our data to bring all the predictor values into a space that is unitless. This transformation of data brings everything to a similar scale, which makes it easier for the classifiers to \"learn\" the data. \n"
      ],
      "id": "7a17354c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X=0.1+(X-np.min(X,axis=0))/(np.max(X,axis=0)-np.min(X,axis=0))"
      ],
      "id": "cf26bec6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation\n",
        "\n",
        "The correlation output below shows an inverse correlation (= -1) between % of Women in the occupation and % of Men in the occupation (which is to be expected). Since we need to maintain independence among the predictor variables, I will drop % of Men in the occupation (Men (%)) to prevent the model from overcounting similar features.\n"
      ],
      "id": "497d36f2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corr = X.corr()\n",
        "print(corr)\t"
      ],
      "id": "d830d9ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The correlation matrix heatmap below as well as the multivariable pair plots reflect the previous correlation output. Again, there is an inverse correlation between employment percent relative standard error (EMP_PRSE) and mean annual salary percent relative standard error (MEAN_PRSE).\n"
      ],
      "id": "2453b3f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.set_theme(style=\"white\")\n",
        "f, ax = plt.subplots(figsize=(11, 9))  # Set up the matplotlib figure\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True) \t# Generate a custom diverging colormap\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n",
        "        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "plt.show();"
      ],
      "id": "d9b3ceee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.pairplot(df.iloc[:,0:5], hue='Target')\n",
        "plt.show()"
      ],
      "id": "530af73e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X.drop(columns=['Men (%)'], inplace=True)"
      ],
      "id": "3c80a736",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Tuning\n",
        "\n",
        "### Pre-processing and Helper Functions\n"
      ],
      "id": "2675bc1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = np.ascontiguousarray(X)\n",
        "NDIM = X.shape[1]"
      ],
      "id": "9eb883a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## helper functions\n",
        "import sklearn.cluster\n",
        "\n",
        "# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) \n",
        "# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\n",
        "def maximize_silhouette(X,algo=\"birch\",nmax=15,i_plot=False):\n",
        "    # LOOP OVER HYPER-PARAM\n",
        "    params=[]; sil_scores=[]\n",
        "    name = algo\n",
        "    sil_max=-10\n",
        "    for param in range(2,nmax+1):\n",
        "        if(algo==\"birch\"):\n",
        "            name = \"Birch\"\n",
        "            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n",
        "            labels=model.predict(X)\n",
        "\n",
        "        if(algo==\"ag\"):\n",
        "            name = \"Agglomerative\"\n",
        "            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param, linkage='ward').fit(X)\n",
        "            labels=model.labels_\n",
        "\n",
        "        if(algo==\"dbscan\"):\n",
        "            name = \"DBSCAN\"\n",
        "            param=0.5*(param-1)\n",
        "            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n",
        "            labels=model.labels_\n",
        "\n",
        "        if(algo==\"kmeans\"):\n",
        "            name = \"K-Means\"\n",
        "            model = sklearn.cluster.KMeans(n_clusters=param, random_state=1234).fit(X)\n",
        "            labels=model.predict(X)\n",
        "            \n",
        "        if(algo==\"ms\"):\n",
        "            name = \"Mean Shift\"\n",
        "            model = sklearn.cluster.MeanShift().fit(X)\n",
        "            labels=model.predict(X)\n",
        "\n",
        "        try:\n",
        "            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n",
        "            params.append(param)\n",
        "        except:\n",
        "            continue \n",
        "        \n",
        "        if(sil_scores[-1]>sil_max):\n",
        "            opt_param=param\n",
        "            sil_max=sil_scores[-1]\n",
        "            opt_labels=labels\n",
        "\n",
        "    print(\"OPTIMAL PARAMETER =\",opt_param)\n",
        "    print(\"Silhouette Coefficient =\",sil_max)\n",
        "\n",
        "    if(i_plot):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.set_title(\"Silhouette Scores based on # of Clusters (%s)\" % name)\n",
        "        ax.plot(params, sil_scores, \"-o\")  \n",
        "        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n",
        "        plt.show()\n",
        "\n",
        "    return opt_labels"
      ],
      "id": "2a5a3dde",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot(x,opt_labels,title=''):\n",
        "\tif(NDIM==2):\n",
        "\t\tfig, ax = plt.subplots()\n",
        "\t\tsp=ax.scatter(x[:,0], x[:,1],c=opt_labels,marker=\".\", cmap=\"viridis\")\n",
        "\t\tplt.colorbar(sp)\n",
        "\n",
        "\tif(NDIM==3):\n",
        "\t\tfig = plt.figure()\n",
        "\t\tax = fig.add_subplot(projection='3d')\n",
        "\t\tsp=ax.scatter(x[:,0],x[:,1],x[:,2],c=opt_labels,marker=\".\", cmap=\"viridis\")\n",
        "\t\tplt.colorbar(sp)\n",
        "\n",
        "\t#DO PCA TO VISUALIZE\n",
        "\tif(NDIM>3):\n",
        "\t\tfrom sklearn.decomposition import PCA\n",
        "\t\tpca = PCA(n_components=3)\n",
        "\t\tpca.fit(x)\n",
        "\t\tY=pca.fit_transform(x)\n",
        "\t\tfig = plt.figure()\n",
        "\t\tax = fig.add_subplot(projection='3d')\n",
        "\t\tsp=ax.scatter(Y[:,0],Y[:,1],Y[:,2],c=opt_labels,marker=\".\", cmap=\"viridis\")\n",
        "\t\tplt.colorbar(sp)\n",
        "\tplt.title(title)\n",
        "\tplt.show()"
      ],
      "id": "33709af4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## ref: https://notebook.community/DistrictDataLabs/yellowbrick/examples/gokriznastic/Iris%20-%20clustering%20example\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "def plot_silhouette(X,nmax=10):\n",
        "    for param in range(2,nmax+1):\n",
        "        model = sklearn.cluster.KMeans(n_clusters=param, random_state=1234).fit(X)\n",
        "        silhouette = SilhouetteVisualizer(model)\n",
        "        silhouette.fit(X)\n",
        "        silhouette.show()  "
      ],
      "id": "dd9dc0f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Means\n",
        "\n",
        "### Hyperparameter Tuning \n",
        "\n",
        "The plot below shows the silhouette coefficient for varying numbers of clusters. The silhouette coefficient peaks at k=3, which would be our optimal number of clusters.\n"
      ],
      "id": "ef199558"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# KMEANS\n",
        "opt_labels=maximize_silhouette(X,algo=\"kmeans\",i_plot=True)"
      ],
      "id": "50a73de1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The silhouette plots below also show similar results with the silhouette scores peaking when k=3.\n"
      ],
      "id": "34dd8bd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_silhouette(X, nmax=6)"
      ],
      "id": "44b2cad3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Results\n",
        "\n",
        "Given the optimal hyperparameter, we cluster our data below with K-Means.\n"
      ],
      "id": "de7d5b6c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot(X,opt_labels,\"K-Means Clustering\")"
      ],
      "id": "4fb152df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DBSCAN\n",
        "\n",
        "DBSCAN hyperparameter tuning is slightly different from K-Means and Hierarchical, which focuses on the number of clusters. Rather, DBSCAN parameter optimization relies on epsilon (EPS), which dictates how close points have to be to each other to be considered a part of a cluster, and min_samples, which dictates how many points are needed to form a cluster.\n",
        "\n",
        "### Hyperparameter Tuning \n",
        "\n",
        "The plot below shows the number of data points in our data set and varying epsilon values. Epsilon in the plot below is labeled as \"Distance,\" since epsilon is a measure of maximum distance between two points that are in the same cluster. Here, we're looking for the point of maximum curvature, which seems to be around eps=[0.15, 0.2].\n"
      ],
      "id": "c2d0079c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Finding optimal EPS\n",
        "## ref: https://towardsdatascience.com/how-to-use-dbscan-effectively-ed212c02e62\n",
        "model = sklearn.neighbors.NearestNeighbors()\n",
        "model = model.fit(X)\n",
        "\n",
        "distances, indices = model.kneighbors(X)\n",
        "distances = np.sort(distances[:,4], axis=0)\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.plot(distances)\n",
        "plt.title(\"EPS Tuning\")\n",
        "plt.xlabel(\"Points\")\n",
        "plt.ylabel(\"Distance\")"
      ],
      "id": "63e7c982",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the precise eps value, we perform the elbow (or knee, in this case) method to find the point of maximum curvature, which is around 0.147.\n"
      ],
      "id": "25db3a9e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from kneed import KneeLocator\n",
        "\n",
        "i = np.arange(len(distances))\n",
        "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "knee.plot_knee()\n",
        "plt.title(\"EPS Tuning (with knee point)\")\n",
        "plt.xlabel(\"Points\")\n",
        "plt.ylabel(\"Distance\")\n",
        "\n",
        "eps = distances[knee.knee]\n",
        "print(eps)"
      ],
      "id": "9c650395",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we take the natural log of the total number of observations in our data set to get the number of min_samples.\n"
      ],
      "id": "6f6ffaec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Finding optimal min_samples\n",
        "## ref: https://stackoverflow.com/questions/12893492/choosing-eps-and-minpts-for-dbscan-r\n",
        "\n",
        "min_samples = round(np.log(df.shape[0]))"
      ],
      "id": "038916f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Results\n",
        "\n",
        "We then create the DBSCAN model with our optimal parameters and get a respectable silhouette coefficient.\n"
      ],
      "id": "8ae08984"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = sklearn.cluster.DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
        "opt_labels=model.labels_\n",
        "sklearn.metrics.silhouette_score(X,opt_labels)"
      ],
      "id": "880df3b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the optimal hyperparameters, we cluster our data below.\n"
      ],
      "id": "a273f27f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot(X,opt_labels,'DBSCAN Clustering')"
      ],
      "id": "13fc03c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hierarchical (Agglomerative)\n",
        "\n",
        "### Hyperparameter Tuning \n",
        "\n",
        "The plot below shows the silhouette coefficient for varying numbers of clusters. The silhouette coefficient peaks at k=4, which would be our optimal number of clusters.\n"
      ],
      "id": "7abb8574"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# AGGLOMERATIVE CLUSTERING\n",
        "opt_labels=maximize_silhouette(X,algo=\"ag\",i_plot=True)"
      ],
      "id": "4e4ecac4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Results\n",
        "\n",
        "Given the optimal hyperparameter, we cluster our data below with Agglomerative clustering.\n"
      ],
      "id": "9a50712c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot(X,opt_labels,'Agglomerative Clustering')"
      ],
      "id": "ed8a596c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also represent the optimized Agglomerative clustering as a tree, with the purple line representing a \"cutoff\" point for the optimal number of clusters.\n"
      ],
      "id": "29aa6021"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "link = linkage(X, method='ward')\n",
        "dend = dendrogram(link)\n",
        "plt.axhline(c='purple',linestyle='--', y=2.5) \n",
        "plt.title(\"Euclidean Distance Dendogram\")\n",
        "plt.xticks([]) # clear clutter from x axis ticks"
      ],
      "id": "c611f9f5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}