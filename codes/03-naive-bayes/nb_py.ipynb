{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and re-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELOAD FILE AND PRETEND THAT IS OUR STARTING POINT \n",
    "df=pd.read_csv('../../data/00-raw-data/wiki-crawl-results.csv')  \n",
    "print(df.shape)\n",
    "\n",
    "#CONVERT FROM STRING LABELS TO INTEGERS \n",
    "labels=[]; #y1=[]; y2=[]\n",
    "y1=[]\n",
    "for label in df[\"label\"]:\n",
    "    if label not in labels:\n",
    "        labels.append(label)\n",
    "        print(\"index =\",len(labels)-1,\": label =\",label)\n",
    "    for i in range(0,len(labels)):\n",
    "        if(label==labels[i]):\n",
    "            y1.append(i)\n",
    "y1=np.array(y1)\n",
    "\n",
    "# CONVERT DF TO LIST OF STRINGS \n",
    "corpus=df[\"text\"].to_list()\n",
    "y2=df[\"sentiment\"].to_numpy()\n",
    "\n",
    "print(\"number of text chunks = \",len(corpus))\n",
    "print(corpus[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE COUNT VECTORIZER\n",
    "# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n",
    "# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "vectorizer=CountVectorizer(min_df=0.001)   \n",
    "\n",
    "# RUN COUNT VECTORIZER ON OUR COURPUS \n",
    "Xs  =  vectorizer.fit_transform(corpus)   \n",
    "X=np.array(Xs.todense())\n",
    "\n",
    "#CONVERT TO ONE-HOT VECTORS\n",
    "maxs=np.max(X,axis=0)\n",
    "X=np.ceil(X/maxs)\n",
    "\n",
    "# DOUBLE CHECK \n",
    "print(X.shape,y1.shape,y2.shape)\n",
    "print(\"DATA POINT-0:\",X[0,0:10],\"y1 =\",y1[0],\"  y2 =\",y2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE TO PARTITION DATASET INTO TRAINING-TEST\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_ratio=0.2\n",
    "\n",
    "# SPLIT ARRAYS OR MATRICES INTO RANDOM TRAIN AND TEST SUBSETS.\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y1, test_size=test_ratio, random_state=0)\n",
    "y_train=y_train.flatten()\n",
    "y_test=y_test.flatten()\n",
    "\n",
    "print(\"x_train.shape\t\t:\",x_train.shape)\n",
    "print(\"y_train.shape\t\t:\",y_train.shape)\n",
    "\n",
    "print(\"X_test.shape\t\t:\",x_test.shape)\n",
    "print(\"y_test.shape\t\t:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions\n",
    "def report(y,ypred):\n",
    "      #ACCURACY COMPUTE \n",
    "      print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n",
    "      print(\"Number of mislabeled points out of a total %d points = %d\"\n",
    "            % (y.shape[0], (y != ypred).sum()))\n",
    "\n",
    "def print_model_summary():\n",
    "      # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n",
    "      yp_train = model.predict(x_train)\n",
    "      yp_test = model.predict(x_test)\n",
    "\n",
    "      print(\"ACCURACY CALCULATION\\n\")\n",
    "\n",
    "      print(\"TRAINING SET:\")\n",
    "      report(y_train,yp_train)\n",
    "\n",
    "      print(\"\\nTEST SET (UNTRAINED DATA):\")\n",
    "      report(y_test,yp_test)\n",
    "\n",
    "      print(\"\\nCHECK FIRST 20 PREDICTIONS\")\n",
    "      print(\"TRAINING SET:\")\n",
    "      print(y_train[0:20])\n",
    "      print(yp_train[0:20])\n",
    "      print(\"ERRORS:\",yp_train[0:20]-y_train[0:20])\n",
    "\n",
    "      print(\"\\nTEST SET (UNTRAINED DATA):\")\n",
    "      print(y_test[0:20])\n",
    "      print(yp_test[0:20])\n",
    "      print(\"ERRORS:\",yp_test[0:20]-y_test[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# INITIALIZE MODEL \n",
    "model = MultinomialNB()\n",
    "\n",
    "# TRAIN MODEL \n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "# PRINT REPORT USING UTILITY FUNCTION ABOVE\n",
    "print_model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/#:~:text=You%20can%20create%20the%20confusion,False%20Negatives%2C%20and%20True%20negatives.\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy_score = accuracy_score(y_test,y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Confusion Matrix (percentages)\n",
    "ax = sns.heatmap(cm / np.sum(cm), annot=True, fmt='.2%', cmap='Reds')\n",
    "\n",
    "ax.set_title('Confusion Matrix with Naive Bayes for Wikipedia Crawl Sentences\\n (Percentages)')\n",
    "ax.set_xlabel('\\nPredicted Sentence Category')\n",
    "ax.set_ylabel('Actual Sentence Category ')\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels([\"Men's Rights\", \"Women's Rights\"])\n",
    "ax.yaxis.set_ticklabels([\"Men's Rights\", \"Women's Rights\"])\n",
    "txt=\"Accuracy Score: {:0.2f}\".format(accuracy_score)\n",
    "plt.figtext(0.44, -.1, txt, wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix (raw)\n",
    "ax = sns.heatmap(cm, annot=True, cmap='Reds')\n",
    "\n",
    "ax.set_title('Confusion Matrix with Naive Bayes for Wikipedia Crawl Sentences\\n (Counts)')\n",
    "ax.set_xlabel('\\nPredicted Sentence Category')\n",
    "ax.set_ylabel('Actual Sentence Category ')\n",
    "plt.figtext(0.44, -.1, txt, wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels([\"Men's Rights\", \"Women's Rights\"])\n",
    "ax.yaxis.set_ticklabels([\"Men's Rights\", \"Women's Rights\"])\n",
    "\n",
    "## Display the visual\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eadb65a309ebe0989826d71328347ef52d129ca1bd9af521e4d4f3d539f018a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
