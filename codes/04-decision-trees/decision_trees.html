<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>decision_trees</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="decision_trees_files/libs/clipboard/clipboard.min.js"></script>
<script src="decision_trees_files/libs/quarto-html/quarto.js"></script>
<script src="decision_trees_files/libs/quarto-html/popper.min.js"></script>
<script src="decision_trees_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="decision_trees_files/libs/quarto-html/anchor.min.js"></script>
<link href="decision_trees_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="decision_trees_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="decision_trees_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="decision_trees_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="decision_trees_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#decision-trees-dts" id="toc-decision-trees-dts" class="nav-link active" data-scroll-target="#decision-trees-dts">Decision Trees (DTs)</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#theory" id="toc-theory" class="nav-link" data-scroll-target="#theory">Theory</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#decision-tree-classification" id="toc-decision-tree-classification" class="nav-link" data-scroll-target="#decision-tree-classification">Decision Tree Classification</a></li>
  <li><a href="#decision-tree-classification-results" id="toc-decision-tree-classification-results" class="nav-link" data-scroll-target="#decision-tree-classification-results">Decision Tree Classification Results</a></li>
  <li><a href="#decision-tree-regression" id="toc-decision-tree-regression" class="nav-link" data-scroll-target="#decision-tree-regression">Decision Tree Regression</a></li>
  <li><a href="#decision-tree-regression-results" id="toc-decision-tree-regression-results" class="nav-link" data-scroll-target="#decision-tree-regression-results">Decision Tree Regression Results</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  </ul></li>
  <li><a href="#random-forests-rfs" id="toc-random-forests-rfs" class="nav-link" data-scroll-target="#random-forests-rfs">Random Forests (RFs)</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1">Introduction</a></li>
  <li><a href="#theory-1" id="toc-theory-1" class="nav-link" data-scroll-target="#theory-1">Theory</a></li>
  <li><a href="#methods-1" id="toc-methods-1" class="nav-link" data-scroll-target="#methods-1">Methods</a>
  <ul class="collapse">
  <li><a href="#random-forest-classification" id="toc-random-forest-classification" class="nav-link" data-scroll-target="#random-forest-classification">Random Forest Classification</a></li>
  <li><a href="#random-forest-classification-results" id="toc-random-forest-classification-results" class="nav-link" data-scroll-target="#random-forest-classification-results">Random Forest Classification Results</a></li>
  </ul></li>
  <li><a href="#conclusions-1" id="toc-conclusions-1" class="nav-link" data-scroll-target="#conclusions-1">Conclusions</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">



<section id="decision-trees-dts" class="level1">
<h1>Decision Trees (DTs)</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The data set that will be used for Decision Tree classification is from the BLS data. This data set contains information on occupations and their respective employment and wages. Our goal is to determine a job type based on its employment and wage metrics. We will also apply Decision Tree regression to another BLS data set. This data set contains information on the number of hours worked by sex and by occupation. Our goal is to determine the average number of hours worked by an employee based on employee sex, job type, and the number of employees who worked 35+ hours per week. For the specific code that achieved these data sets, please refer to the data cleaning section. For further detail about these data sets, please refer to the exploring data section.</p>
</section>
<section id="theory" class="level2">
<h2 class="anchored" data-anchor-id="theory">Theory</h2>
<p>Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. As the name suggests, DTs creates a tree-like structure consisting of decisions in the form of conditional (if) statements and their respective resulting consequences. DTs are conceptually similar to a flowchart, where each node of the tree represents a “Yes” or “No” test on a particular attribute, each branch represents the outcome of the test, and each leaf node represents a particular class to which a particular item would most likely belong.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="dt_example.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Xoriant / “Decision Trees for Classification: A Machine Learning Algorithm”</figcaption><p></p>
</figure>
</div>
<p>For example, the above image is an example of a very elementary DT to decide (or classify) whether or not a person is fit. At the very top node (the root node), we typically have a question that can split the data set as “efficiently” as possible (although the metric for efficiency will differ based on the criterion). In this case, we first ask if a person is less than 30 years old. If so, we then ask if this person eats a lot of pizzas. If the answer is yes, then we’d conclude that this person is unfit; if the answer is no, we’d conclude that this person is fit. However, if this person is older than 30 years old, we ask if this person exercises in the morning. If the answer is yes, then we conclude that the person is fit; otherwise, we conclude that the person is unfit.</p>
<p>As per the example, simply asking a person’s age, frequency of pizza consumption, and exercise habits is probably insufficient to truly decide if a person is fit. This is a simplistic workflow, but the algorithms used by the DTs search for good tree rules that attempts to best classify the given dataset. However, finding the optimal tree is very difficult due to the infinite number of possible decision trees that can be constructed given the attributes of a data set. As such, DTs can be a good, computationally cheap starting point to find interesting trends within the data, which can later be further explored with more detailed and computationally expensive methods.</p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<section id="decision-tree-classification" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree-classification">Decision Tree Classification</h3>
<section id="imports" class="level4">
<h4 class="anchored" data-anchor-id="imports">Imports</h4>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="load-data" class="level4">
<h4 class="anchored" data-anchor-id="load-data">Load Data</h4>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'../../data/01-modified-data/occupations_detailed_(employment_and_wage).csv'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">## drop unneeded column created from read_csv</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.iloc[:, <span class="dv">1</span>:]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">## rename</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>df.rename(columns<span class="op">=</span>{<span class="st">'TOT_EMP'</span>:<span class="st">'Total Employment'</span>, <span class="st">'EMP_PRSE'</span>: <span class="st">'Employment PRSE'</span>, <span class="st">'A_MEAN'</span>:<span class="st">'Annual Mean Wage'</span>, <span class="st">'MEAN_PRSE'</span>:<span class="st">'Mean Wage PRSE'</span>}, inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="separate-predictor-and-response-variables" class="level4">
<h4 class="anchored" data-anchor-id="separate-predictor-and-response-variables">Separate Predictor and Response Variables</h4>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Y="Target" COLUMN and X="everything else"</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.iloc[:, <span class="dv">2</span>:<span class="dv">6</span>]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df.iloc[:, <span class="dv">7</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="normalization" class="level4">
<h4 class="anchored" data-anchor-id="normalization">Normalization</h4>
<p>Since our predictors (X) consist of employment rates and mean annual wages, we first normalize our data to bring all the predictor values into a space that is unitless. This transformation of data brings everything to a similar scale, which makes it easier for the DT algorithm to “learn” the data.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span><span class="fl">0.1</span><span class="op">+</span>(X<span class="op">-</span>np.<span class="bu">min</span>(X,axis<span class="op">=</span><span class="dv">0</span>))<span class="op">/</span>(np.<span class="bu">max</span>(X,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">-</span>np.<span class="bu">min</span>(X,axis<span class="op">=</span><span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="class-distribution" class="level4">
<h4 class="anchored" data-anchor-id="class-distribution">Class Distribution</h4>
<section id="numerical-eda" class="level5">
<h5 class="anchored" data-anchor-id="numerical-eda">Numerical EDA</h5>
<p>As shown in the output below, our target class is heavily imbalanced. Since this imbalance can skew the way the data is split into training and test sets, we will later stratify the data so the proportion of values in the training and test sets also reflect this imbalance.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Target'</span>].value_counts(ascending<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="113">
<pre><code>Legal Occupations                                              15
Building and Grounds Cleaning and Maintenance Occupations      18
Farming, Fishing, and Forestry Occupations                     24
Community and Social Service Occupations                       26
Healthcare Support Occupations                                 27
Food Preparation and Serving Related Occupations               33
Computer and Mathematical Occupations                          36
Sales and Related Occupations                                  42
Protective Service Occupations                                 43
Arts, Design, Entertainment, Sports, and Media Occupations     55
Business and Financial Operations Occupations                  58
Personal Care and Service Occupations                          60
Architecture and Engineering Occupations                       61
Management Occupations                                         73
Installation, Maintenance, and Repair Occupations              75
Life, Physical, and Social Science Occupations                 79
Transportation and Material Moving Occupations                 91
Educational Instruction and Library Occupations                97
Healthcare Practitioners and Technical Occupations            102
Construction and Extraction Occupations                       103
Office and Administrative Support Occupations                 109
Production Occupations                                        167
Name: Target, dtype: int64</code></pre>
</div>
</div>
</section>
<section id="multivariable-pair-plot" class="level5">
<h5 class="anchored" data-anchor-id="multivariable-pair-plot">Multivariable Pair Plot</h5>
<p>As mentioned before, the distributions of the target class is heavily imbalanced. Again, we can see this visually represented in the density plots in the correlation multivariable pair plot below.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>sns.pairplot(df.iloc[:, <span class="dv">2</span>:<span class="dv">7</span>], hue<span class="op">=</span><span class="st">'Target'</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-7-output-1.png" width="1409" height="943"></p>
</div>
</div>
</section>
</section>
<section id="baseline-random-classifier" class="level4">
<h4 class="anchored" data-anchor-id="baseline-random-classifier">Baseline: Random Classifier</h4>
<p>In order to have some baseline to compare our DT’s performance, we defined a random classifier below.</p>
<section id="define-random-classifier-function" class="level5">
<h5 class="anchored" data-anchor-id="define-random-classifier-function">Define Random Classifier Function</h5>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_fscore_support</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_classifier(y_data):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    ypred<span class="op">=</span>[]<span class="op">;</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    max_label<span class="op">=</span>np.<span class="bu">max</span>(y_data)<span class="op">;</span> <span class="co">#print(max_label)</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(y_data)):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        ypred.append(<span class="bu">int</span>(np.floor((max_label<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>np.random.uniform(<span class="dv">0</span>,<span class="dv">1</span>))))</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-----RANDOM CLASSIFIER-----"</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"count of prediction:"</span>,Counter(ypred).values()) <span class="co"># counts the elements' frequency</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"probability of prediction:"</span>,np.fromiter(Counter(ypred).values(), dtype<span class="op">=</span><span class="bu">float</span>)<span class="op">/</span><span class="bu">len</span>(y_data)) <span class="co"># counts the elements' frequency</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"accuracy"</span>,accuracy_score(y_data, ypred))</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"precision, recall, fscore,"</span>,precision_recall_fscore_support(y_data, ypred))</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>random_classifier(Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>-----RANDOM CLASSIFIER-----
count of prediction: dict_values([56, 65, 67, 61, 58, 53, 64, 72, 53, 57, 49, 66, 53, 76, 60, 60, 62, 55, 63, 61, 64, 55, 64])
probability of prediction: [0.04017217 0.04662841 0.04806313 0.04375897 0.04160689 0.03802009
 0.04591105 0.05164993 0.03802009 0.04088953 0.03515065 0.04734577
 0.03802009 0.05451937 0.04304161 0.04304161 0.04447633 0.03945481
 0.04519369 0.04375897 0.04591105 0.03945481 0.04591105]
accuracy 0.03802008608321377
precision, recall, fscore, (array([0.        , 0.06666667, 0.03125   , 0.046875  , 0.03571429,
       0.05454545, 0.01886792, 0.01818182, 0.03773585, 0.03225806,
       0.05970149, 0.01754386, 0.01886792, 0.01587302, 0.03076923,
       0.08163265, 0.        , 0.10344828, 0.03278689, 0.03278689,
       0.01388889, 0.06666667, 0.078125  ]), array([0.        , 0.05479452, 0.03448276, 0.08333333, 0.03278689,
       0.03797468, 0.03846154, 0.06666667, 0.02061856, 0.03636364,
       0.03921569, 0.03703704, 0.02325581, 0.03030303, 0.11111111,
       0.06666667, 0.        , 0.05504587, 0.08333333, 0.01941748,
       0.01333333, 0.0239521 , 0.05494505]), array([0.        , 0.06015038, 0.03278689, 0.06      , 0.03418803,
       0.04477612, 0.02531646, 0.02857143, 0.02666667, 0.03418803,
       0.04733728, 0.02380952, 0.02083333, 0.02083333, 0.04819277,
       0.0733945 , 0.        , 0.07185629, 0.04705882, 0.02439024,
       0.01360544, 0.03524229, 0.06451613]), array([  0,  73,  58,  36,  61,  79,  26,  15,  97,  55, 102,  27,  43,
        33,  18,  60,  42, 109,  24, 103,  75, 167,  91], dtype=int64))</code></pre>
</div>
</div>
<p>Based on the output above, we can see that accuracy of the random classifier is 0.04, which is around what we’d expect from randomly taking guesses for 22 target classes. We can also see that the precision, recall, and f-scores from the random classifier are all below 0.1.</p>
</section>
</section>
<section id="feature-selection" class="level4">
<h4 class="anchored" data-anchor-id="feature-selection">Feature Selection</h4>
<section id="correlation" class="level5">
<h5 class="anchored" data-anchor-id="correlation">Correlation</h5>
<p>The correlation output below shows a strong positive correlation (&gt; 0.8) between employment percent relative standard error (EMP_PRSE) and mean annual salary percent relative standard error (MEAN_PRSE). Since we need to maintain independence among the predictor variables, I will drop employment percent relative standard error (EMP_PRSE) to prevent the model from overcounting similar features.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> X.corr()</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(corr) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                  Total Employment  Employment PRSE  Annual Mean Wage  \
Total Employment          1.000000        -0.235767         -0.072640   
Employment PRSE          -0.235767         1.000000          0.115099   
Annual Mean Wage         -0.072640         0.115099          1.000000   
Mean Wage PRSE           -0.190195         0.801454          0.158494   

                  Mean Wage PRSE  
Total Employment       -0.190195  
Employment PRSE         0.801454  
Annual Mean Wage        0.158494  
Mean Wage PRSE          1.000000  </code></pre>
</div>
</div>
</section>
<section id="correlation-matrix-heatmap" class="level5">
<h5 class="anchored" data-anchor-id="correlation-matrix-heatmap">Correlation Matrix Heatmap</h5>
<p>The correlation matrix heatmap below reflects the previous correlation output. Again, there is a strong positive correlation between Employment percent relative standard error (PRSE) and mean annual salary PRSE.</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">9</span>))  <span class="co"># Set up the matplotlib figure</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.diverging_palette(<span class="dv">230</span>, <span class="dv">20</span>, as_cmap<span class="op">=</span><span class="va">True</span>)     <span class="co"># Generate a custom diverging colormap</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the heatmap with the mask and correct aspect ratio</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr,  cmap<span class="op">=</span>cmap, vmin<span class="op">=-</span><span class="dv">1</span>, vmax<span class="op">=</span><span class="dv">1</span>, center<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        square<span class="op">=</span><span class="va">True</span>, linewidths<span class="op">=</span><span class="fl">.5</span>, cbar_kws<span class="op">=</span>{<span class="st">"shrink"</span>: <span class="fl">.5</span>})</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-10-output-1.png" width="805" height="707"></p>
</div>
</div>
</section>
<section id="remove-correlated-features" class="level5">
<h5 class="anchored" data-anchor-id="remove-correlated-features">Remove Correlated Features</h5>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>X.drop(columns<span class="op">=</span>[<span class="st">'Employment PRSE'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="split-data" class="level4">
<h4 class="anchored" data-anchor-id="split-data">Split Data</h4>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PARTITION THE DATASET INTO TRAINING AND </span><span class="al">TEST</span><span class="co"> SETS</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>test_ratio<span class="op">=</span><span class="fl">0.2</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(X, Y, test_size<span class="op">=</span>test_ratio, random_state<span class="op">=</span><span class="dv">1234</span>, stratify<span class="op">=</span>Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="training-the-model-w-default-parameters" class="level4">
<h4 class="anchored" data-anchor-id="training-the-model-w-default-parameters">Training the Model w/ Default Parameters</h4>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train </span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tree.DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.fit(x_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="check-the-results" class="level4">
<h4 class="anchored" data-anchor-id="check-the-results">Check the Results</h4>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># USE THE MODEL TO MAKE PREDICTIONS FOR THE TRAINING AND </span><span class="al">TEST</span><span class="co"> SET </span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>yp_train <span class="op">=</span> model.predict(x_train)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>yp_test <span class="op">=</span> model.predict(x_test)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># GENERATES A CONFUSION MATRIX PLOT AND PRINTS MODEL PERFORMANCE METRICS</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> confusion_plot(y_data, y_pred):    </span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    cm <span class="op">=</span> confusion_matrix(y_data, y_pred)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    disp.plot()</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'ACCURACY:'</span>, accuracy_score(y_data, y_pred))</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RECALL:'</span>, recall_score(y_data, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'PRECISION:'</span>, precision_score(y_data, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"------TRAINING------"</span>)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>confusion_plot(y_train,yp_train)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"------TEST------"</span>)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>confusion_plot(y_test,yp_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>------TRAINING------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>ACCURACY: 1.0
RECALL: 1.0
PRECISION: 1.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-14-output-3.png" width="512" height="428"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>------TEST------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>ACCURACY: 0.5304659498207885
RECALL: 0.5304659498207885
PRECISION: 0.5477572404466717</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-14-output-6.png" width="504" height="428"></p>
</div>
</div>
<p>As shown in the correlation matrices above for both the training and test data sets, we can see that the DT resulted in a perfect fit for the training set but a much less adequate fit for the test set. Such a drastic difference in the accuracy, recall, and precision scores between the training and test sets suggest significant overfitting of the model (which is a notable characteristic of DTs).</p>
</section>
<section id="visualize-the-tree" class="level4">
<h4 class="anchored" data-anchor-id="visualize-the-tree">Visualize the Tree</h4>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># VISUALIZE THE DECISION TREE</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>regr <span class="op">=</span> DecisionTreeRegressor(random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> regr.fit(x_train, y_train)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_tree(model, X, Y):</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">25</span>,<span class="dv">20</span>))</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> tree.plot_tree(model, </span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>                    feature_names<span class="op">=</span>X.columns,  </span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>                    class_names<span class="op">=</span>Y.name,</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>                    filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>plot_tree(model, X, Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-15-output-1.png" width="1884" height="1469"></p>
</div>
</div>
</section>
<section id="model-tuning" class="level4">
<h4 class="anchored" data-anchor-id="model-tuning">Model Tuning</h4>
<p>As mentioned previously, the model with default parameters resulted in a heavily overfit DT. In order to find a more well-rounded model, we will perform model tuning.</p>
<section id="hyperparameter-tuning" class="level5">
<h5 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning</h5>
<p>First, we loop over possible hyperparameter values, ranging from 1 to 50, keeping track of the training and test sets’ accuracy and recall scores for each hyperparameter value. We then create plots for accuracy and recall scores for the training and test sets to identify which number of layers for the DT would result in an optimal model.</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LOOP OVER POSSIBLE HYPER-PARAMETERS VALUES</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>test_results<span class="op">=</span>[]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>train_results<span class="op">=</span>[]</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">51</span>):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tree.DecisionTreeClassifier(max_depth<span class="op">=</span>num_layer, random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.fit(x_train, y_train)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(y_pred.shape)</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test, average<span class="op">=</span><span class="st">'weighted'</span>)])</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train, average<span class="op">=</span><span class="st">'weighted'</span>)])</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [el[<span class="dv">0</span>] <span class="cf">for</span> el <span class="kw">in</span> test_results]</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>          </span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> [el[<span class="dv">1</span>] <span class="cf">for</span> el <span class="kw">in</span> test_results]</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>test_rec <span class="op">=</span> [el[<span class="dv">2</span>] <span class="cf">for</span> el <span class="kw">in</span> test_results]</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> [el[<span class="dv">1</span>] <span class="cf">for</span> el <span class="kw">in</span> train_results]</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>train_rec <span class="op">=</span> [el[<span class="dv">2</span>] <span class="cf">for</span> el <span class="kw">in</span> train_results]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="find-optimal-hyperparameter" class="level5">
<h5 class="anchored" data-anchor-id="find-optimal-hyperparameter">Find Optimal Hyperparameter</h5>
<p>Based on the plots below, we can narrow down the best hyperparameter value for our model as somewhere between 2-4, since both accuracy and recall scores begin to diverge more and more dramatically beginning from max_depth=5. To avoid our original default model’s issue of overfitting, we will proceed with our optimal hyperparameter value of max_depth=4.</p>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># GENERATE PLOTS TO IDENTIFY OPTIMAL HYPERPARAMETER</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gen_plots(x, train, test):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    plt.plot(x,train, c<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    plt.scatter(x,train,c<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    plt.plot(x,test,c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    plt.scatter(x,test,c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Number of layers in decision tree (max_depth)"</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    plt.show()<span class="op">;</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"ACCURACY: Training (blue) and Test (red)"</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>gen_plots(layers, train_acc, test_acc)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"RECALL: Training (blue) and Test (red)"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>gen_plots(layers, train_rec, test_rec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-17-output-1.png" width="593" height="428"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-17-output-2.png" width="593" height="428"></p>
</div>
</div>
<p>To numerically validate our intuition from the previous graphs, we can once again see from the output below that hyperparameter values 1-4 have training and test mean absolute errors that are relatively similar. However, from value 5, we can see that the difference between the two mean absolute errors begins to grow rather rapidly. As such, we can confirm our prior intution and proceed with max_depth=4 as our optimal hyperparameter value.</p>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_percentage_error</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>hyper_param<span class="op">=</span>[]</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>train_error<span class="op">=</span>[]</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>test_error<span class="op">=</span>[]</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">23</span>):</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># INITIALIZE MODEL </span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span>i, random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># TRAIN MODEL </span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    model.fit(x_train,y_train)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># OUTPUT PREDICTIONS FOR TRAINING AND </span><span class="al">TEST</span><span class="co"> SET </span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    yp_train <span class="op">=</span> model.predict(x_train)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    yp_test <span class="op">=</span> model.predict(x_test)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># GET MAE</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    err1<span class="op">=</span>mean_absolute_error(y_train, yp_train) </span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    err2<span class="op">=</span>mean_absolute_error(y_test, yp_test) </span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    hyper_param.append(i)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    train_error.append(err1)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    test_error.append(err2)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"hyperparam ="</span>,i)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" train error:"</span>,err1)</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" test error:"</span> ,err2)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" error diff:"</span> ,<span class="bu">abs</span>(err2<span class="op">-</span>err1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>hyperparam = 1
 train error: 4.463065114511777
 test error: 4.525002503281594
 error diff: 0.06193738876981669
hyperparam = 2
 train error: 4.3514877977490265
 test error: 4.527379781988495
 error diff: 0.1758919842394686
hyperparam = 3
 train error: 4.114299945653438
 test error: 4.294305673744442
 error diff: 0.1800057280910039
hyperparam = 4
 train error: 3.907945039135891
 test error: 4.290328630067027
 error diff: 0.3823835909311355
hyperparam = 5
 train error: 3.61239509839229
 test error: 4.220391413301806
 error diff: 0.6079963149095162
hyperparam = 6
 train error: 3.300386564232065
 test error: 4.144748169209051
 error diff: 0.844361604976986
hyperparam = 7
 train error: 2.9027863786450046
 test error: 4.026013042626716
 error diff: 1.1232266639817117
hyperparam = 8
 train error: 2.4427671341010213
 test error: 3.93220702217629
 error diff: 1.4894398880752688
hyperparam = 9
 train error: 2.078309577363488
 test error: 3.9458210788382906
 error diff: 1.8675115014748025
hyperparam = 10
 train error: 1.740467589395823
 test error: 3.7729787686688967
 error diff: 2.0325111792730737
hyperparam = 11
 train error: 1.4699770747639724
 test error: 3.635844174813886
 error diff: 2.1658671000499137
hyperparam = 12
 train error: 1.1812153879860412
 test error: 3.424796182365575
 error diff: 2.2435807943795334
hyperparam = 13
 train error: 0.9255702560690605
 test error: 3.484268674325243
 error diff: 2.5586984182561823
hyperparam = 14
 train error: 0.7033187890500375
 test error: 3.1646908361636896
 error diff: 2.461372047113652
hyperparam = 15
 train error: 0.5043140452267332
 test error: 3.362102364569158
 error diff: 2.8577883193424247
hyperparam = 16
 train error: 0.3421915731913094
 test error: 3.1484610570632077
 error diff: 2.8062694838718985
hyperparam = 17
 train error: 0.27764653383487464
 test error: 3.341446208112875
 error diff: 3.063799674278
hyperparam = 18
 train error: 0.22379488672762216
 test error: 3.124530636627411
 error diff: 2.9007357498997886</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>hyperparam = 19
 train error: 0.14344650864830236
 test error: 3.2419354838709675
 error diff: 3.098488975222665
hyperparam = 20
 train error: 0.10951953875720692
 test error: 3.118168629458952
 error diff: 3.008649090701745
hyperparam = 21
 train error: 0.056950672645739914
 test error: 3.0782556750298684
 error diff: 3.0213050023841284
hyperparam = 22
 train error: 0.029417040358744394
 test error: 3.00752688172043
 error diff: 2.9781098413616856</code></pre>
</div>
</div>
</section>
</section>
<section id="train-optimal-model" class="level4">
<h4 class="anchored" data-anchor-id="train-optimal-model">Train Optimal Model</h4>
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">#### TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train </span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tree.DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.fit(x_train, y_train)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"------TRAINING------"</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>confusion_plot(y_train,yp_train)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"------TEST------"</span>)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>confusion_plot(y_test,yp_test)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>regr <span class="op">=</span> DecisionTreeRegressor(random_state<span class="op">=</span><span class="dv">1234</span>, max_depth<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> regr.fit(x_train, y_train)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>plot_tree(model, X, Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>------TRAINING------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>ACCURACY: 0.2825112107623318
RECALL: 0.2825112107623318
PRECISION: 0.2994247417856412</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-19-output-3.png" width="512" height="430"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>------TEST------
ACCURACY: 0.24731182795698925
RECALL: 0.24731182795698925
PRECISION: 0.19358094653793578</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-19-output-5.png" width="504" height="428"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-19-output-6.png" width="1879" height="1469"></p>
</div>
</div>
</section>
</section>
<section id="decision-tree-classification-results" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree-classification-results">Decision Tree Classification Results</h3>
<p>The confusion matrices above show a large decrease in the accuracy, recall, and precision scores for both the training and test sets compared to the metrics from our original model with default parameters; however, we are now no longer overfitting. The accuracy, recall, and precision scores are around 0.3. These trends are also reflected in the new DT. By reducing the depth of the tree, we inevitably reduce the accuracy, recall, and precision scores of the model; however, we also reduce overfitting and increase generalizability.</p>
</section>
<section id="decision-tree-regression" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree-regression">Decision Tree Regression</h3>
<section id="imports-1" class="level4">
<h4 class="anchored" data-anchor-id="imports-1">Imports</h4>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="load-data-1" class="level4">
<h4 class="anchored" data-anchor-id="load-data-1">Load Data</h4>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'../../data/01-modified-data/hours_worked_(by_sex_and_by_occupation)_final.csv'</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co">## drop unneeded rows</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[df[<span class="st">'sex'</span>] <span class="op">!=</span> <span class="st">'B'</span>]</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co"># convert from long to wide</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.pivot(df, index<span class="op">=</span>[<span class="st">'Category'</span>, <span class="st">'sex'</span>], columns<span class="op">=</span>[<span class="st">'Measure'</span>], values<span class="op">=</span><span class="st">'Value'</span>).reset_index()</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co"># create numerical representations for occupation categories</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Category_num'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>df.iloc[<span class="dv">0</span>:<span class="dv">2</span>, <span class="dv">7</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>df.iloc[<span class="dv">2</span>:<span class="dv">4</span>, <span class="dv">7</span>] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>df.iloc[<span class="dv">4</span>:<span class="dv">6</span>, <span class="dv">7</span>] <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>df.iloc[<span class="dv">6</span>:<span class="dv">8</span>, <span class="dv">7</span>] <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>df.iloc[<span class="dv">8</span>:, <span class="dv">7</span>] <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co"># convert sex to numerical (0 = m, 1 = f)</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'sex'</span>] <span class="op">=</span> df[<span class="st">'sex'</span>].replace(<span class="st">'F'</span>, <span class="dv">1</span>).replace(<span class="st">'M'</span>, <span class="dv">0</span>)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>df.rename(columns<span class="op">=</span>{<span class="st">'Average hrs worked among all workers'</span> : <span class="st">'Target'</span>, <span class="st">'Average hrs worked among full-time workers'</span>:<span class="st">'Full-time hours worked'</span>, <span class="st">'No. people at work'</span>:<span class="st">'# People at Work'</span>, <span class="st">'No. people who worked 35+ hrs'</span>:<span class="st">'&gt;35 hrs at work'</span>, <span class="st">'No. people who worked &lt; 35hrs'</span>:<span class="st">'&lt;35 hrs at work'</span>}, inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="separate-predictor-and-response-variables-1" class="level4">
<h4 class="anchored" data-anchor-id="separate-predictor-and-response-variables-1">Separate Predictor and Response Variables</h4>
<div class="cell" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.iloc[:, [<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>]]</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[<span class="st">'Target'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="normalization-1" class="level4">
<h4 class="anchored" data-anchor-id="normalization-1">Normalization</h4>
<p>Since our predictors (X) consist of hours worked by employees, total number of employees working in job types, and numerical representations for sex and job types, we first normalize our data to bring all the predictor values into a space that is unitless. This transformation of data brings everything to a similar scale, which makes it easier for the DT algorithm to “learn” the data.</p>
<div class="cell" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span><span class="fl">0.1</span><span class="op">+</span>(X<span class="op">-</span>np.<span class="bu">min</span>(X,axis<span class="op">=</span><span class="dv">0</span>))<span class="op">/</span>(np.<span class="bu">max</span>(X,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">-</span>np.<span class="bu">min</span>(X,axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>Y<span class="op">=</span><span class="fl">0.1</span><span class="op">+</span>(Y<span class="op">-</span>np.<span class="bu">min</span>(Y,axis<span class="op">=</span><span class="dv">0</span>))<span class="op">/</span>(np.<span class="bu">max</span>(Y,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">-</span>np.<span class="bu">min</span>(Y,axis<span class="op">=</span><span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="class-distribution-1" class="level4">
<h4 class="anchored" data-anchor-id="class-distribution-1">Class Distribution</h4>
<section id="numerical-eda-1" class="level5">
<h5 class="anchored" data-anchor-id="numerical-eda-1">Numerical EDA</h5>
<p>As shown in the output below, our target class is roughly balanced.</p>
<div class="cell" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Target'</span>].value_counts(ascending<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="131">
<pre><code>37.1    1
42.0    1
33.0    1
40.7    1
35.7    1
39.5    1
38.6    1
37.7    1
36.6    2
40.6    2
Name: Target, dtype: int64</code></pre>
</div>
</div>
</section>
<section id="multivariable-pair-plot-1" class="level5">
<h5 class="anchored" data-anchor-id="multivariable-pair-plot-1">Multivariable Pair Plot</h5>
<div class="cell" data-execution_count="24">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>sns.pairplot(df.iloc[:, <span class="dv">1</span>:<span class="dv">7</span>], hue<span class="op">=</span><span class="st">'Target'</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-25-output-1.png" width="1272" height="1179"></p>
</div>
</div>
</section>
</section>
<section id="feature-selection-1" class="level4">
<h4 class="anchored" data-anchor-id="feature-selection-1">Feature Selection</h4>
<section id="correlation-1" class="level5">
<h5 class="anchored" data-anchor-id="correlation-1">Correlation</h5>
<p>The correlation output below shows a strong negative correlation (&lt; -0.8) between sex and the average hours worked among full-time workers. Additionally, we can see a strong positive correlation among the number of people at work, the number of people who worked 35+ hours, and the number of people who worked &lt; 35 hours. Since we need to maintain independence among the predictor variables, I will drop the average hours worked among full-time workers, the number of people at work, and the number of people who worked &lt; 35 hours.</p>
<div class="cell" data-execution_count="25">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> X.corr()</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(corr) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Measure                          sex  Full-time hours worked  \
Measure                                                        
sex                     1.000000e+00               -0.884945   
Full-time hours worked -8.849447e-01                1.000000   
# People at Work       -6.824094e-02                0.208849   
&gt;35 hrs at work        -1.371153e-01                0.287486   
&lt;35 hrs at work         1.806626e-01               -0.087910   
Category_num           -7.447602e-17               -0.259927   

Measure                 # People at Work  &gt;35 hrs at work  &lt;35 hrs at work  \
Measure                                                                      
sex                            -0.068241        -0.137115         0.180663   
Full-time hours worked          0.208849         0.287486        -0.087910   
# People at Work                1.000000         0.994119         0.924555   
&gt;35 hrs at work                 0.994119         1.000000         0.877853   
&lt;35 hrs at work                 0.924555         0.877853         1.000000   
Category_num                    0.316673         0.263533         0.473204   

Measure                 Category_num  
Measure                               
sex                    -7.447602e-17  
Full-time hours worked -2.599267e-01  
# People at Work        3.166733e-01  
&gt;35 hrs at work         2.635328e-01  
&lt;35 hrs at work         4.732042e-01  
Category_num            1.000000e+00  </code></pre>
</div>
</div>
</section>
<section id="correlation-matrix-heatmap-1" class="level5">
<h5 class="anchored" data-anchor-id="correlation-matrix-heatmap-1">Correlation Matrix Heatmap</h5>
<p>The correlation matrix heatmap below reflects the previous correlation output. Again, there is a strong negative correlation between sex and the average hours worked among full-time workers, and a strong positive correlation among the number of people at work, the number of people who worked 35+ hours, and the number of people who worked &lt; 35 hours.</p>
<div class="cell" data-execution_count="26">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">9</span>))  <span class="co"># Set up the matplotlib figure</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.diverging_palette(<span class="dv">230</span>, <span class="dv">20</span>, as_cmap<span class="op">=</span><span class="va">True</span>)     <span class="co"># Generate a custom diverging colormap</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the heatmap with the mask and correct aspect ratio</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr,  cmap<span class="op">=</span>cmap, vmin<span class="op">=-</span><span class="dv">1</span>, vmax<span class="op">=</span><span class="dv">1</span>, center<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        square<span class="op">=</span><span class="va">True</span>, linewidths<span class="op">=</span><span class="fl">.5</span>, cbar_kws<span class="op">=</span>{<span class="st">"shrink"</span>: <span class="fl">.5</span>})</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-27-output-1.png" width="960" height="853"></p>
</div>
</div>
</section>
<section id="remove-correlated-features-1" class="level5">
<h5 class="anchored" data-anchor-id="remove-correlated-features-1">Remove Correlated Features</h5>
<div class="cell" data-execution_count="27">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>X.drop(X.columns[[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>]], axis<span class="op">=</span><span class="dv">1</span>, inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="split-data-1" class="level4">
<h4 class="anchored" data-anchor-id="split-data-1">Split Data</h4>
<div class="cell" data-execution_count="28">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PARTITION THE DATASET INTO TRAINING AND </span><span class="al">TEST</span><span class="co"> SETS</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>test_ratio<span class="op">=</span><span class="fl">0.2</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(X, Y, test_size<span class="op">=</span>test_ratio, random_state<span class="op">=</span><span class="dv">1234</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="model-tuning-1" class="level4">
<h4 class="anchored" data-anchor-id="model-tuning-1">Model Tuning</h4>
<p>A known issue with DTs is the proneness of the model to overfit training data. In order to find a more well-rounded model, we will perform model tuning.</p>
<section id="hyperparameter-tuning-1" class="level5">
<h5 class="anchored" data-anchor-id="hyperparameter-tuning-1">Hyperparameter Tuning</h5>
<p>First, we loop over possible hyperparameter values, ranging from 1 to 10, keeping track of the training and test sets’ mean absolute errors (MAE) for each hyperparameter value. Ideally, we want to find the hyperparameter that minimizes the MAE for both training and test sets while not having the respective MAE values diverge too severely from each other.</p>
<p>From the output below, we can clearly see that the model begins overfitting quickly, with the MAE difference between training and test set becoming greater than 10 fold by min_samples_split=3.</p>
<div class="cell" data-execution_count="29">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_percentage_error</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="co"># HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS </span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>hyper_param<span class="op">=</span>[]</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>train_error<span class="op">=</span>[]</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>test_error<span class="op">=</span>[]</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co"># LOOP OVER HYPER-PARAM</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">11</span>):</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># INITIALIZE MODEL </span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span>i, random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># TRAIN MODEL </span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    model.fit(x_train,y_train)</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># OUTPUT PREDICTIONS FOR TRAINING AND </span><span class="al">TEST</span><span class="co"> SET </span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>    yp_train <span class="op">=</span> model.predict(x_train)</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>    yp_test <span class="op">=</span> model.predict(x_test)</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># shift=1+np.min(y_train) #add shift to remove division by zero </span></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>    err1<span class="op">=</span>mean_absolute_error(y_train, yp_train) </span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>    err2<span class="op">=</span>mean_absolute_error(y_test, yp_test) </span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))</span></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))</span></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>    hyper_param.append(i)</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>    train_error.append(err1)</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>    test_error.append(err2)</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"hyperparam ="</span>,i)</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" train error:"</span>,err1)</span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" test error:"</span> ,err2)</span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" error diff:"</span> ,<span class="bu">abs</span>(err2<span class="op">-</span>err1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>hyperparam = 1
 train error: 0.12888888888888891
 test error: 0.1903703703703705
 error diff: 0.06148148148148158
hyperparam = 2
 train error: 0.04814814814814808
 test error: 0.3111111111111112
 error diff: 0.2629629629629632
hyperparam = 3
 train error: 0.012345679012345671
 test error: 0.28888888888888903
 error diff: 0.27654320987654335
hyperparam = 4
 train error: 0.0
 test error: 0.3074074074074075
 error diff: 0.3074074074074075
hyperparam = 5
 train error: 0.0
 test error: 0.3074074074074075
 error diff: 0.3074074074074075
hyperparam = 6
 train error: 0.0
 test error: 0.3074074074074075
 error diff: 0.3074074074074075
hyperparam = 7
 train error: 0.0
 test error: 0.3074074074074075
 error diff: 0.3074074074074075
hyperparam = 8
 train error: 0.0
 test error: 0.3074074074074075
 error diff: 0.3074074074074075
hyperparam = 9
 train error: 0.0
 test error: 0.3074074074074075
 error diff: 0.3074074074074075
hyperparam = 10
 train error: 0.0
 test error: 0.3074074074074075
 error diff: 0.3074074074074075</code></pre>
</div>
</div>
</section>
<section id="convergence-plot" class="level5">
<h5 class="anchored" data-anchor-id="convergence-plot">Convergence Plot</h5>
<p>Our numerical intuition from above can also be visualized with a convergence plot. Again, we can see quite rapid overfitting occurring, and there really doesn’t seem to be a “safe” choice of hyperparameter that avoids overfitting outright; however, our intution of min_samples_split=2 seems to be a moderate choice to avoid the dramatic overfitting that begins to occur from min_samples_split=3.</p>
<div class="cell" data-execution_count="30">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>plt.plot(hyper_param,train_error ,linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>plt.plot(hyper_param,test_error ,linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Depth of tree (max depth)"</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Training (black) and test (blue) MAE (error)"</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-31-output-1.png" width="601" height="428"></p>
</div>
</div>
</section>
</section>
<section id="train-optimal-model-1" class="level4">
<h4 class="anchored" data-anchor-id="train-optimal-model-1">Train Optimal Model</h4>
<div class="cell" data-execution_count="31">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># INITIALIZE MODEL </span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>model.fit(x_train,y_train)                     <span class="co"># TRAIN MODEL </span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="co"># OUTPUT PREDICTIONS FOR TRAINING AND </span><span class="al">TEST</span><span class="co"> SET </span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>yp_train <span class="op">=</span> model.predict(x_train)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>yp_test <span class="op">=</span> model.predict(x_test)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>err1<span class="op">=</span>mean_absolute_error(y_train, yp_train) </span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>err2<span class="op">=</span>mean_absolute_error(y_test, yp_test) </span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" train error:"</span>,err1)</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" test error:"</span> ,err2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> train error: 0.04814814814814808
 test error: 0.3111111111111112</code></pre>
</div>
</div>
</section>
<section id="plot-tree" class="level4">
<h4 class="anchored" data-anchor-id="plot-tree">Plot Tree</h4>
<div class="cell" data-execution_count="32">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_tree(model):</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">25</span>,<span class="dv">20</span>))</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> tree.plot_tree(model, </span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>                    feature_names<span class="op">=</span>X.columns,  </span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>                    class_names<span class="op">=</span>Y.name,</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>                    filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>plot_tree(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-33-output-1.png" width="1879" height="1469"></p>
</div>
</div>
</section>
<section id="parity-plot" class="level4">
<h4 class="anchored" data-anchor-id="parity-plot">Parity Plot</h4>
<div class="cell" data-execution_count="33">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>plt.plot(y_train,yp_train ,<span class="st">"o"</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>plt.plot(y_test,yp_test ,<span class="st">"o"</span>, color<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>plt.plot(y_train,y_train ,<span class="st">"-"</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"y_data"</span>)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y_pred (blue=test)(black=Train)"</span>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-34-output-1.png" width="593" height="428"></p>
</div>
</div>
</section>
<section id="linear-regression" class="level4">
<h4 class="anchored" data-anchor-id="linear-regression">Linear Regression</h4>
<div class="cell" data-execution_count="34">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LINEAR REGRESSION </span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression().fit(X, Y)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="co"># OUTPUT PREDICTIONS FOR TRAINING AND </span><span class="al">TEST</span><span class="co"> SET </span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>yp_train <span class="op">=</span> model.predict(x_train)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>yp_test <span class="op">=</span> model.predict(x_test)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>plt.plot(y_train,yp_train ,<span class="st">"o"</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>plt.plot(y_test,yp_test ,<span class="st">"o"</span>, color<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>plt.plot(y_train,y_train,<span class="st">"-"</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"y_data"</span>)</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y_pred (blue=test)(black=Train)"</span>)</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>err1<span class="op">=</span><span class="fl">100.0</span><span class="op">*</span>np.mean(np.absolute((yp_train<span class="op">-</span>y_train)<span class="op">/</span>y_train))</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>err2<span class="op">=</span><span class="fl">100.0</span><span class="op">*</span>np.mean(np.absolute((yp_test<span class="op">-</span>y_test)<span class="op">/</span>y_test))</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" train error:"</span>,err1)</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" test error:"</span> ,err2)</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> train error: 8.40027080950112
 test error: 51.06983733731278</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-35-output-2.png" width="593" height="428"></p>
</div>
</div>
</section>
</section>
<section id="decision-tree-regression-results" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree-regression-results">Decision Tree Regression Results</h3>
<p>The new tree from the optimized model primarily focuses on splitting our data based on Category_num, which corresponds to the occupation categories. The parity plot for our model doesn’t fall closely to the desired y=x line, which suggests that the fit for this model definitely has room for improvement. Additionally, the linear regression shows a test error lower than training error, which suggests sampling bias.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>For the DT Classification, our model, given a job type’s employment and wage metrics, can classify the associated job type with an accuracy of 0.28. Based on the DT classification results, we can see that our optimized model outperforms the random classifier by about 7 times (accuracy=0.04 vs.&nbsp;accuracy=0.28), which is quite a large improvement from classifying completely randomly. Nonetheless, the model can definitely see some improvements. Most notably, DTs do not perform well on unbalanced data sets. As such, obtaining more data to create a balanced data set may improve the classifier’s performance. Additionally, having more predictor variables with which the classifier could work may generally help fine-tune the classifier further.</p>
<p>For the DT regression, our model can determine the average number of hours worked by an employee based on employee sex, job type, and the number of employees who worked 35+ hours per week. Overall, it appears that the model isn’t performing very well; however, it’s even difficult to try and truly quantify the extent to which the model is poorly classifying the average number of hours worked by an employee based on employee sex, job type, and the number of employees who worked 35+ hours per week. The most glaring issue with the model at hand stems at the extremely small sample size, that is further divided into training and test sets. Having more predictor variables and a generally larger sample size with which the regressor could work may help fine-tune the regressor even more and create some a foundation from which we can begin to more accurately interpret the performance of the model.</p>
</section>
</section>
<section id="random-forests-rfs" class="level1">
<h1>Random Forests (RFs)</h1>
<section id="introduction-1" class="level2">
<h2 class="anchored" data-anchor-id="introduction-1">Introduction</h2>
<p>The data set that will be used for Random Forest classification is the same BLS data that was used for Decision Tree classification. Once again, our goal is to determine a job type based on its employment and wage metrics.</p>
</section>
<section id="theory-1" class="level2">
<h2 class="anchored" data-anchor-id="theory-1">Theory</h2>
<p>Random Forests (RFs) are an ensemble learning method used for classification and regression. As the name suggests, RFs are composed of multiple decision trees. The algorithm works based on the “consensus of the masses” by creating a forest of uncorrelated decision trees that consider only a select subset of features, which is a notable difference from decision trees, which considers all possible feature splits. As such, the output from RFs is the class selected by the majority of the individual decision trees.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="rf_example.svg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">TIBCO / “What is a Random Forest?”</figcaption><p></p>
</figure>
</div>
<p>For example, the above image is an example of the RF classification process. From the dataset, multiple decision trees (hence “forest”) are generated, with each individual tree spitting out a classification result. Then the individual results from each tree are gathered, and a majority vote is conducted to decide on the final result (hence “consensus of the masses”). Since RFs are the consensus vote of multiple decision trees, we’d also generally expect the RF classification and regression results to outperform a single decision tree.</p>
</section>
<section id="methods-1" class="level2">
<h2 class="anchored" data-anchor-id="methods-1">Methods</h2>
<section id="random-forest-classification" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-classification">Random Forest Classification</h3>
<section id="imports-2" class="level4">
<h4 class="anchored" data-anchor-id="imports-2">Imports</h4>
<div class="cell" data-execution_count="35">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="load-data-2" class="level4">
<h4 class="anchored" data-anchor-id="load-data-2">Load Data</h4>
<div class="cell" data-execution_count="36">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'../../data/01-modified-data/occupations_detailed_(employment_and_wage).csv'</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co">## drop unneeded column created from read_csv</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.iloc[:, <span class="dv">1</span>:]</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="co">## rename</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>df.rename(columns<span class="op">=</span>{<span class="st">'TOT_EMP'</span>:<span class="st">'Total Employment'</span>, <span class="st">'EMP_PRSE'</span>: <span class="st">'Employment PRSE'</span>, <span class="st">'A_MEAN'</span>:<span class="st">'Annual Mean Wage'</span>, <span class="st">'MEAN_PRSE'</span>:<span class="st">'Mean Wage PRSE'</span>}, inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="separate-predictor-and-response-variables-2" class="level4">
<h4 class="anchored" data-anchor-id="separate-predictor-and-response-variables-2">Separate Predictor and Response Variables</h4>
<div class="cell" data-execution_count="37">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Y="Target" COLUMN and X="everything else"</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.iloc[:, <span class="dv">2</span>:<span class="dv">6</span>]</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df.iloc[:, <span class="dv">7</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="normalization-2" class="level4">
<h4 class="anchored" data-anchor-id="normalization-2">Normalization</h4>
<p>Since our predictors (X) consist of employment rates and mean annual wages, we first normalize our data to bring all the predictor values into a space that is unitless. This transformation of data brings everything to a similar scale, which makes it easier for the DT algorithm to “learn” the data.</p>
<div class="cell" data-execution_count="38">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span><span class="fl">0.1</span><span class="op">+</span>(X<span class="op">-</span>np.<span class="bu">min</span>(X,axis<span class="op">=</span><span class="dv">0</span>))<span class="op">/</span>(np.<span class="bu">max</span>(X,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">-</span>np.<span class="bu">min</span>(X,axis<span class="op">=</span><span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="class-distribution-2" class="level4">
<h4 class="anchored" data-anchor-id="class-distribution-2">Class Distribution</h4>
<section id="numerical-eda-2" class="level5">
<h5 class="anchored" data-anchor-id="numerical-eda-2">Numerical EDA</h5>
<p>As shown in the output below, our target class is heavily imbalanced. Since this imbalance can skew the way the data is split into training and test sets, we will later stratify the data so the proportion of values in the training and test sets also reflect this imbalance.</p>
<div class="cell" data-execution_count="39">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Target'</span>].value_counts(ascending<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="147">
<pre><code>Legal Occupations                                              15
Building and Grounds Cleaning and Maintenance Occupations      18
Farming, Fishing, and Forestry Occupations                     24
Community and Social Service Occupations                       26
Healthcare Support Occupations                                 27
Food Preparation and Serving Related Occupations               33
Computer and Mathematical Occupations                          36
Sales and Related Occupations                                  42
Protective Service Occupations                                 43
Arts, Design, Entertainment, Sports, and Media Occupations     55
Business and Financial Operations Occupations                  58
Personal Care and Service Occupations                          60
Architecture and Engineering Occupations                       61
Management Occupations                                         73
Installation, Maintenance, and Repair Occupations              75
Life, Physical, and Social Science Occupations                 79
Transportation and Material Moving Occupations                 91
Educational Instruction and Library Occupations                97
Healthcare Practitioners and Technical Occupations            102
Construction and Extraction Occupations                       103
Office and Administrative Support Occupations                 109
Production Occupations                                        167
Name: Target, dtype: int64</code></pre>
</div>
</div>
</section>
<section id="multivariable-pair-plot-2" class="level5">
<h5 class="anchored" data-anchor-id="multivariable-pair-plot-2">Multivariable Pair Plot</h5>
<p>As mentioned before, the distributions of the target class is heavily imbalanced. Again, we can see this visually represented in the density plots in the correlation multivariable pair plot below.</p>
<div class="cell" data-execution_count="40">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>sns.pairplot(df.iloc[:, <span class="dv">2</span>:<span class="dv">7</span>], hue<span class="op">=</span><span class="st">'Target'</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-41-output-1.png" width="1409" height="943"></p>
</div>
</div>
</section>
</section>
<section id="baseline-random-classifier-1" class="level4">
<h4 class="anchored" data-anchor-id="baseline-random-classifier-1">Baseline: Random Classifier</h4>
<p>In order to have some baseline to compare our DT’s performance, we defined a random classifier below.</p>
<section id="define-random-classifier-function-1" class="level5">
<h5 class="anchored" data-anchor-id="define-random-classifier-function-1">Define Random Classifier Function</h5>
<div class="cell" data-execution_count="41">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_fscore_support</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_classifier(y_data):</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    ypred<span class="op">=</span>[]<span class="op">;</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    max_label<span class="op">=</span>np.<span class="bu">max</span>(y_data)<span class="op">;</span> <span class="co">#print(max_label)</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(y_data)):</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>        ypred.append(<span class="bu">int</span>(np.floor((max_label<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>np.random.uniform(<span class="dv">0</span>,<span class="dv">1</span>))))</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-----RANDOM CLASSIFIER-----"</span>)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"count of prediction:"</span>,Counter(ypred).values()) <span class="co"># counts the elements' frequency</span></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"probability of prediction:"</span>,np.fromiter(Counter(ypred).values(), dtype<span class="op">=</span><span class="bu">float</span>)<span class="op">/</span><span class="bu">len</span>(y_data)) <span class="co"># counts the elements' frequency</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"accuracy"</span>,accuracy_score(y_data, ypred))</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"precision, recall, fscore,"</span>,precision_recall_fscore_support(y_data, ypred))</span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>random_classifier(Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>-----RANDOM CLASSIFIER-----
count of prediction: dict_values([56, 65, 67, 61, 58, 53, 64, 72, 53, 57, 49, 66, 53, 76, 60, 60, 62, 55, 63, 61, 64, 55, 64])
probability of prediction: [0.04017217 0.04662841 0.04806313 0.04375897 0.04160689 0.03802009
 0.04591105 0.05164993 0.03802009 0.04088953 0.03515065 0.04734577
 0.03802009 0.05451937 0.04304161 0.04304161 0.04447633 0.03945481
 0.04519369 0.04375897 0.04591105 0.03945481 0.04591105]
accuracy 0.03802008608321377
precision, recall, fscore, (array([0.        , 0.06666667, 0.03125   , 0.046875  , 0.03571429,
       0.05454545, 0.01886792, 0.01818182, 0.03773585, 0.03225806,
       0.05970149, 0.01754386, 0.01886792, 0.01587302, 0.03076923,
       0.08163265, 0.        , 0.10344828, 0.03278689, 0.03278689,
       0.01388889, 0.06666667, 0.078125  ]), array([0.        , 0.05479452, 0.03448276, 0.08333333, 0.03278689,
       0.03797468, 0.03846154, 0.06666667, 0.02061856, 0.03636364,
       0.03921569, 0.03703704, 0.02325581, 0.03030303, 0.11111111,
       0.06666667, 0.        , 0.05504587, 0.08333333, 0.01941748,
       0.01333333, 0.0239521 , 0.05494505]), array([0.        , 0.06015038, 0.03278689, 0.06      , 0.03418803,
       0.04477612, 0.02531646, 0.02857143, 0.02666667, 0.03418803,
       0.04733728, 0.02380952, 0.02083333, 0.02083333, 0.04819277,
       0.0733945 , 0.        , 0.07185629, 0.04705882, 0.02439024,
       0.01360544, 0.03524229, 0.06451613]), array([  0,  73,  58,  36,  61,  79,  26,  15,  97,  55, 102,  27,  43,
        33,  18,  60,  42, 109,  24, 103,  75, 167,  91], dtype=int64))</code></pre>
</div>
</div>
<p>Based on the output above, we can see that accuracy of the random classifier is 0.04, which is around what we’d expect from randomly taking guesses for 22 target classes. We can also see that the precision, recall, and f-scores from the random classifier are all below 0.1.</p>
</section>
</section>
<section id="feature-selection-2" class="level4">
<h4 class="anchored" data-anchor-id="feature-selection-2">Feature Selection</h4>
<section id="correlation-2" class="level5">
<h5 class="anchored" data-anchor-id="correlation-2">Correlation</h5>
<p>The correlation output below shows a strong positive correlation (&gt; 0.8) between employment percent relative standard error (EMP_PRSE) and mean annual salary percent relative standard error (MEAN_PRSE). Since we need to maintain independence among the predictor variables, I will drop employment percent relative standard error (EMP_PRSE) to prevent the model from overcounting similar features.</p>
<div class="cell" data-execution_count="42">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> X.corr()</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(corr) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                  Total Employment  Employment PRSE  Annual Mean Wage  \
Total Employment          1.000000        -0.235767         -0.072640   
Employment PRSE          -0.235767         1.000000          0.115099   
Annual Mean Wage         -0.072640         0.115099          1.000000   
Mean Wage PRSE           -0.190195         0.801454          0.158494   

                  Mean Wage PRSE  
Total Employment       -0.190195  
Employment PRSE         0.801454  
Annual Mean Wage        0.158494  
Mean Wage PRSE          1.000000  </code></pre>
</div>
</div>
</section>
<section id="correlation-matrix-heatmap-2" class="level5">
<h5 class="anchored" data-anchor-id="correlation-matrix-heatmap-2">Correlation Matrix Heatmap</h5>
<p>The correlation matrix heatmap below reflects the previous correlation output. Again, there is a strong positive correlation between Employment percent relative standard error (PRSE) and mean annual salary PRSE.</p>
<div class="cell" data-execution_count="43">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">9</span>))  <span class="co"># Set up the matplotlib figure</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.diverging_palette(<span class="dv">230</span>, <span class="dv">20</span>, as_cmap<span class="op">=</span><span class="va">True</span>)     <span class="co"># Generate a custom diverging colormap</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the heatmap with the mask and correct aspect ratio</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr,  cmap<span class="op">=</span>cmap, vmin<span class="op">=-</span><span class="dv">1</span>, vmax<span class="op">=</span><span class="dv">1</span>, center<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        square<span class="op">=</span><span class="va">True</span>, linewidths<span class="op">=</span><span class="fl">.5</span>, cbar_kws<span class="op">=</span>{<span class="st">"shrink"</span>: <span class="fl">.5</span>})</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-44-output-1.png" width="805" height="707"></p>
</div>
</div>
</section>
<section id="remove-correlated-features-2" class="level5">
<h5 class="anchored" data-anchor-id="remove-correlated-features-2">Remove Correlated Features</h5>
<div class="cell" data-execution_count="44">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>X.drop(columns<span class="op">=</span>[<span class="st">'Employment PRSE'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="split-data-2" class="level4">
<h4 class="anchored" data-anchor-id="split-data-2">Split Data</h4>
<div class="cell" data-execution_count="45">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PARTITION THE DATASET INTO TRAINING AND </span><span class="al">TEST</span><span class="co"> SETS</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>test_ratio<span class="op">=</span><span class="fl">0.2</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(X, Y, test_size<span class="op">=</span>test_ratio, random_state<span class="op">=</span><span class="dv">1234</span>, stratify<span class="op">=</span>Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="training-the-model-w-default-parameters-1" class="level4">
<h4 class="anchored" data-anchor-id="training-the-model-w-default-parameters-1">Training the Model w/ Default Parameters</h4>
<div class="cell" data-execution_count="46">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TRAIN A SKLEARN RANDOM FOREST MODEL ON x_train,y_train </span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.fit(x_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="check-the-results-1" class="level4">
<h4 class="anchored" data-anchor-id="check-the-results-1">Check the Results</h4>
<div class="cell" data-execution_count="47">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># USE THE MODEL TO MAKE PREDICTIONS FOR THE TRAINING AND </span><span class="al">TEST</span><span class="co"> SET </span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>yp_train <span class="op">=</span> model.predict(x_train)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>yp_test <span class="op">=</span> model.predict(x_test)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="co"># GENERATES A CONFUSION MATRIX PLOT AND PRINTS MODEL PERFORMANCE METRICS</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> confusion_plot(y_data, y_pred):    </span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>    cm <span class="op">=</span> confusion_matrix(y_data, y_pred)</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>    disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm)</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    disp.plot()</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'ACCURACY:'</span>, accuracy_score(y_data, y_pred))</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RECALL:'</span>, recall_score(y_data, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'PRECISION:'</span>, precision_score(y_data, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"------TRAINING------"</span>)</span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>confusion_plot(y_train,yp_train)</span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"------TEST------"</span>)</span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a>confusion_plot(y_test,yp_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>------TRAINING------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>ACCURACY: 1.0
RECALL: 1.0
PRECISION: 1.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-48-output-3.png" width="512" height="428"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>------TEST------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>ACCURACY: 0.5519713261648745
RECALL: 0.5519713261648745
PRECISION: 0.5619829245441531</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-48-output-6.png" width="516" height="434"></p>
</div>
</div>
<p>As shown in the correlation matrices above for both the training and test data sets, we can see that the RF resulted in a perfect fit for the training set but a much less adequate fit for the test set. Such a drastic difference in the accuracy, recall, and precision scores between the training and test sets suggest significant overfitting of the model (which is a notable characteristic of RFs).</p>
</section>
<section id="visualize-the-tree-1" class="level4">
<h4 class="anchored" data-anchor-id="visualize-the-tree-1">Visualize the Tree</h4>
<div class="cell" data-execution_count="48">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>regr <span class="op">=</span> RandomForestRegressor(random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> regr.fit(x_train, y_train)</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a><span class="co"># VISUALIZE A SINGLE TREE</span></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_tree(model, X, Y):</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">25</span>,<span class="dv">20</span>))</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> tree.plot_tree(model.estimators_[<span class="dv">0</span>], </span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>            feature_names<span class="op">=</span>X.columns,  </span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>            filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>plot_tree(model, X, Y)</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-49-output-1.png" width="1883" height="1469"></p>
</div>
</div>
</section>
<section id="model-tuning-2" class="level4">
<h4 class="anchored" data-anchor-id="model-tuning-2">Model Tuning</h4>
<p>As mentioned previously, the model with default parameters resulted in a heavily overfit RF. In order to find a more well-rounded model, we will perform model tuning.</p>
<section id="hyperparameter-tuning-2" class="level5">
<h5 class="anchored" data-anchor-id="hyperparameter-tuning-2">Hyperparameter Tuning</h5>
<p>First, we loop over possible hyperparameter values, ranging from 1 to 50, keeping track of the training and test sets’ accuracy and recall scores for each hyperparameter value. We then create plots for accuracy and recall scores for the training and test sets to identify which number of layers for the RF would result in an optimal model.</p>
<div class="cell" data-execution_count="49">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LOOP OVER POSSIBLE HYPER-PARAMETERS VALUES</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>test_results<span class="op">=</span>[]</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>train_results<span class="op">=</span>[]</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">51</span>):</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> RandomForestClassifier(max_depth<span class="op">=</span>num_layer, random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.fit(x_train, y_train)</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>    yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>    yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(y_pred.shape)</span></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test, average<span class="op">=</span><span class="st">'weighted'</span>)])</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train, average<span class="op">=</span><span class="st">'weighted'</span>)])</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [el[<span class="dv">0</span>] <span class="cf">for</span> el <span class="kw">in</span> test_results]</span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>          </span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> [el[<span class="dv">1</span>] <span class="cf">for</span> el <span class="kw">in</span> test_results]</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>test_rec <span class="op">=</span> [el[<span class="dv">2</span>] <span class="cf">for</span> el <span class="kw">in</span> test_results]</span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> [el[<span class="dv">1</span>] <span class="cf">for</span> el <span class="kw">in</span> train_results]</span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a>train_rec <span class="op">=</span> [el[<span class="dv">2</span>] <span class="cf">for</span> el <span class="kw">in</span> train_results]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="find-optimal-hyperparameter-visual" class="level5">
<h5 class="anchored" data-anchor-id="find-optimal-hyperparameter-visual">Find Optimal Hyperparameter (Visual)</h5>
<p>Based on the plots below, we can narrow down the best hyperparameter value for our model as somewhere between 2-4, since both accuracy and recall scores begin to diverge more and more dramatically beginning from max_depth=5.</p>
<div class="cell" data-execution_count="50">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># GENERATE PLOTS TO IDENTIFY OPTIMAL HYPERPARAMETER</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gen_plots(x, train, test):</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    plt.plot(x,train, c<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    plt.scatter(x,train,c<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>    plt.plot(x,test,c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    plt.scatter(x,test,c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Number of layers in decision tree (max_depth)"</span>)</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>    plt.show()<span class="op">;</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"ACCURACY: Training (blue) and Test (red)"</span>)</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>gen_plots(layers, train_acc, test_acc)</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"RECALL: Training (blue) and Test (red)"</span>)</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>gen_plots(layers, train_rec, test_rec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-51-output-1.png" width="593" height="428"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-51-output-2.png" width="593" height="428"></p>
</div>
</div>
</section>
<section id="find-optimal-hyperparameter-gridsearch" class="level5">
<h5 class="anchored" data-anchor-id="find-optimal-hyperparameter-gridsearch">Find Optimal Hyperparameter (GridSearch)</h5>
<p>In addition to the max_depth parameter, we also need to find the optimal number of actual DTs that compose the RF, represented by the n_estimators parameter. To do this, we will do a grid search for max_depth values from 1 to 5 and number of DTs ranging from 1 to 100.</p>
<div class="cell" data-execution_count="51">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ref: https://medium.datadriveninvestor.com/random-forest-regression-9871bc9a25eb</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score, GridSearchCV</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rfr_model(X, y):</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Grid-Search</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>    gsc <span class="op">=</span> GridSearchCV(</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>        estimator<span class="op">=</span>RandomForestRegressor(random_state<span class="op">=</span><span class="dv">1234</span>),</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>        param_grid<span class="op">=</span>{</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>            <span class="st">'max_depth'</span>: <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">5</span>),</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">'n_estimators'</span>: <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">101</span>),</span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>        cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>, verbose<span class="op">=</span><span class="dv">0</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a>    grid_result <span class="op">=</span> gsc.fit(X, y)</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a>    best_params <span class="op">=</span> grid_result.best_params_</span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> best_params</span>
<span id="cb75-19"><a href="#cb75-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-20"><a href="#cb75-20" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> rfr_model(X, Y)</span>
<span id="cb75-21"><a href="#cb75-21" aria-hidden="true" tabindex="-1"></a>best_params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="159">
<pre><code>{'max_depth': 4, 'n_estimators': 11}</code></pre>
</div>
</div>
<p>We can see from the output below that the optimal parameters are max_depth=4 and n_estimators=11. Notably, the max_depth parameter matches our prior visual exploration of optimal hyperparameter values.</p>
</section>
</section>
<section id="train-optimal-model-2" class="level4">
<h4 class="anchored" data-anchor-id="train-optimal-model-2">Train Optimal Model</h4>
<div class="cell" data-execution_count="52">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier(max_depth<span class="op">=</span>best_params[<span class="st">'max_depth'</span>], n_estimators<span class="op">=</span>best_params[<span class="st">'n_estimators'</span>], random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.fit(x_train, y_train)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"------TRAINING------"</span>)</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>confusion_plot(y_train,yp_train)</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"------TEST------"</span>)</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>confusion_plot(y_test,yp_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>------TRAINING------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>ACCURACY: 0.3434977578475336
RECALL: 0.3434977578475336
PRECISION: 0.318686493816976</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-53-output-3.png" width="512" height="430"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>------TEST------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>ACCURACY: 0.2939068100358423
RECALL: 0.2939068100358423
PRECISION: 0.2167606731010513</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-53-output-6.png" width="504" height="428"></p>
</div>
</div>
<div class="cell" data-execution_count="53">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># VISUALIZE 5 TREES</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>regr <span class="op">=</span> RandomForestRegressor(random_state<span class="op">=</span><span class="dv">1234</span>, max_depth<span class="op">=</span><span class="dv">4</span>, n_estimators<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> regr.fit(x_train, y_train)</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="co">## ref: https://stackoverflow.com/questions/40155128/plot-trees-for-a-random-forest-in-python-with-scikit-learn</span></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_tree(model, X, Y):</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(nrows <span class="op">=</span> <span class="dv">1</span>,ncols <span class="op">=</span> <span class="dv">5</span>,figsize <span class="op">=</span> (<span class="dv">10</span>,<span class="dv">2</span>), dpi<span class="op">=</span><span class="dv">900</span>)</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">5</span>):</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>        tree.plot_tree(model.estimators_[index],</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>                    feature_names<span class="op">=</span>X.columns, </span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>                    filled <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>                    ax <span class="op">=</span> axes[index])<span class="op">;</span></span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>        axes[index].set_title(<span class="st">'Estimator: '</span> <span class="op">+</span> <span class="bu">str</span>(index), fontsize <span class="op">=</span> <span class="dv">11</span>)</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a>plot_tree(model, X, Y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_trees_files/figure-html/cell-54-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="random-forest-classification-results" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-classification-results">Random Forest Classification Results</h3>
<p>The confusion matrices above show a large decrease in the accuracy, recall, and precision scores for both the training and test sets compared to the metrics from our original model with default parameters; however, we are now no longer overfitting. The accuracy and recall scores are around 0.3, and precision is around 0.22. These trends are also reflected in the RF. With reducing the depth of the tree, we inevitably reduce the accuracy, recall, and precision scores of the model; however, we also reduce overfitting and increase generalizability.</p>
</section>
</section>
<section id="conclusions-1" class="level2">
<h2 class="anchored" data-anchor-id="conclusions-1">Conclusions</h2>
<p>Our RF model, given a job type’s employment and wage metrics, can classify the associated job type with an accuracy of 0.3. Based on the RF classification results, we can see that our optimized model outperforms the random classifier by about 8 times (accuracy=0.04 vs.&nbsp;accuracy=0.3), which is quite a large improvement from classifying completely randomly. Additionally, our RF classification model also outperforms the results from our decision tree classification results (accuracy=0.3 vs.&nbsp;accuracy=0.28). This result is to be expected, since RFs are composed of multiple decision trees.</p>
<p>Nonetheless, the model can definitely see some improvements. Most notably, RFs, since they’re composed of decision trees, also suffer from similar shortcomings, such as poor performance on unbalanced data sets. As such, obtaining more data to create a balanced data set may improve the classifier’s performance. Additionally, having more predictor variables with which the classifier could work may generally help fine-tune the classifier further.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>