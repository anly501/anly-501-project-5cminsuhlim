{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "import networkx as nx \n",
    "\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'employment right woman include non discriminatory access woman job equal pay right woman men equal pay equal benefit equal work openly denied british hong kong government early 1970s leslie wah leung chung 1917 2009 president hong kong chinese civil servant association 1965 68 contributed establishment equal pay men woman including right married woman permanent employee job status woman changed permanent employee temporary employee married thus losing pension benefit even lost job since nurse mostly woman improvement right married woman meant much nursing profession european country married woman could work without consent husband decade ago example france 1965 spain 1975 addition marriage bar practice adopted late 19th century 1970s across many country including austria australia ireland canada switzerland restricted married woman employment many profession key issue towards insuring gender equality workplace respecting maternity right reproductive right woman maternity leave paternity leave country parental leave temporary period absence employment granted immediately childbirth order support mother full recovery grant time care baby different country different rule regarding maternity leave paternity leave parental leave'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELOAD FILE AND PRETEND THAT IS OUR STARTING POINT \n",
    "df=pd.read_csv('./data/00-raw-data/wiki-crawl-results.csv')  \n",
    "corpus=df[\"text\"].to_list()\n",
    "print(df.shape)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#USER PARAM\n",
    "input_path = 'DRACULA.txt'\n",
    "compute_sentiment =\tTrue\t\t\n",
    "sentiment =\t[]\t\t\t#average sentiment of each chunk of text \n",
    "ave_window_size\t= 250\t\t#size of scanning window for moving average\n",
    "\t\t\t\t\t\n",
    "\n",
    "#OUTPUT FILE\n",
    "output='transactions.txt'\n",
    "if os.path.exists(output): \n",
    "    os.remove(output)\n",
    "\n",
    "#INITIALIZE\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "#ADD MORE\n",
    "stopwords = stopwords.words('english')\n",
    "add=['mr','mrs','wa','dr','said','back','could','one','looked','like','know','around','dont']\n",
    "for sp in add: \n",
    "    stopwords.append(sp)\n",
    "\n",
    "def read_and_clean(corpus,START=0,STOP=-1):\n",
    "    global sentiment\n",
    "    \n",
    "    text = ' '.join(corpus)\n",
    "    \n",
    "\t#REMOVE HEADER, AND NEW LINES\n",
    "    text=text.replace(\"'\",'') #wasn't --> wasnt\n",
    "    lines = text.splitlines(); text='';\n",
    "    lines=lines[START:STOP] \n",
    "    for line in lines: text=text+' '+line\n",
    "\n",
    "\t#-----------------------\n",
    "\t#INSERT CODE TO ONLY KEEP CHAR IN string.printable\n",
    "\t#-----------------------\n",
    "    tmp = ''\n",
    "    printable = set(string.printable)\n",
    "    for char in text:\n",
    "        if char in printable:\n",
    "            tmp = tmp + char\n",
    "    text = tmp\n",
    "  \n",
    "\t#BREAK INTO CHUNKS (SENTENCES OR OTHERWISE)\n",
    "    sentences=nltk.tokenize.sent_tokenize(text)  #SENTENCES\n",
    "    \n",
    "    print(\"NUMBER OF SENTENCES FOUND:\",len(sentences)); #print(sentences)\n",
    "\n",
    "\t#CLEAN AND LEMMATIZE\n",
    "    keep='0123456789abcdefghijklmnopqrstuvwxy';\n",
    "    \n",
    "    new_sentences=[]; vocabulary=[]\n",
    "    for sentence in sentences:\n",
    "        new_sentence=''\n",
    "\n",
    "\t\t# REBUILD LEMITIZED SENTENCE\n",
    "        for word in sentence.split():\n",
    "\t\t\t\n",
    "\t\t\t#ONLY KEEP CHAR IN \"keep\"\n",
    "            tmp2=''\n",
    "            for char in word: \n",
    "                if(char in keep): \n",
    "                    tmp2=tmp2+char\n",
    "                else:\n",
    "                    tmp2=tmp2+' '\n",
    "            word=tmp2\n",
    "\n",
    "\t\t\t#-----------------------\n",
    "\t\t\t# INSERT CODE TO LEMMATIZE THE WORDS\n",
    "\t\t\t#-----------------------\n",
    "            new_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "\t\t\t#REMOVE WHITE SPACES\n",
    "            new_word=new_word.replace(' ', '')\n",
    "\n",
    "\t\t\t#BUILD NEW SENTENCE BACK UP\n",
    "            if( new_word not in stopwords):\n",
    "                if(new_sentence==''):\n",
    "                    new_sentence=new_word\n",
    "                else:\n",
    "                    new_sentence=new_sentence+','+new_word\n",
    "                if(new_word not in vocabulary): vocabulary.append(new_word)\n",
    "\n",
    "\t\t#SAVE (LIST OF LISTS)\t\t\n",
    "        new_sentences.append(new_sentence.split(\",\"))\n",
    "\t\t\n",
    "    #SIA\n",
    "    if(compute_sentiment):\n",
    "        #-----------------------\n",
    "        # INSERT CODE TO USE NLTK TO DO SENTIMENT ANALYSIS \n",
    "        #-----------------------\n",
    "        for sentence in new_sentences:\n",
    "            score = sia.polarity_scores(\" \".join(sentence))\n",
    "            sentiment_scores = [score['neg'],score['neu'],score['pos'],score['compound']]\n",
    "            sentiment.append(sentiment_scores)\n",
    "\t\n",
    "    sentiment=np.array(sentiment)\n",
    "    print(\"TOTAL AVERAGE SENTEMENT:\",np.mean(sentiment,axis=0))\n",
    "    print(\"VOCAB LENGTH\",len(vocabulary))\n",
    "    return new_sentences\n",
    "\n",
    "transactions=read_and_clean(corpus,400,-400)\n",
    "print(transactions[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE TO RE-FORMAT THE APRIORI OUTPUT INTO A PANDAS DATA-FRAME WITH COLUMNS \"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"\n",
    "def reformat_results(results):\n",
    "\n",
    "    #CLEAN-UP RESULTS \n",
    "    keep=[]\n",
    "    for i in range(0,len(results)):\n",
    "        # print(\"=====================================\")\n",
    "        # print(results[i])\n",
    "        # print(len(list(results[i])))\n",
    "        for j in range(0,len(list(results[i]))):\n",
    "            # print(results)\n",
    "            if(j>1):\n",
    "                for k in range(0,len(list(results[i][j]))):\n",
    "                    if(len(results[i][j][k][0])!=0):\n",
    "                        #print(len(results[i][j][k][0]),results[i][j][k][0])\n",
    "                        rhs=list(results[i][j][k][0])\n",
    "                        lhs=list(results[i][j][k][1])\n",
    "                        conf=float(results[i][j][k][2])\n",
    "                        lift=float(results[i][j][k][3])\n",
    "                        keep.append([rhs,lhs,supp,conf,supp*conf,lift])\n",
    "                        # keep.append()\n",
    "            if(j==1):\n",
    "                supp=results[i][j]\n",
    "\n",
    "    return pd.DataFrame(keep, columns =[\"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_network(df):\n",
    "    print(df)\n",
    "\n",
    "    #BUILD GRAPH\n",
    "    G = nx.DiGraph()  # DIRECTED\n",
    "    for row in df.iterrows():\n",
    "        # for column in df.columns:\n",
    "        lhs=\"_\".join(row[1][0])\n",
    "        rhs=\"_\".join(row[1][1])\n",
    "        conf=row[1][3]; #print(conf)\n",
    "        if(lhs not in G.nodes): \n",
    "            G.add_node(lhs)\n",
    "        if(rhs not in G.nodes): \n",
    "            G.add_node(rhs)\n",
    "\n",
    "        edge=(lhs,rhs)\n",
    "        if edge not in G.edges:\n",
    "            G.add_edge(lhs, rhs, weight=conf)\n",
    "\n",
    "    # print(G.nodes)\n",
    "    # print(G.edges)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network(G):\n",
    "    #SPECIFIY X-Y POSITIONS FOR PLOTTING\n",
    "    pos=nx.random_layout(G)\n",
    "\n",
    "    #GENERATE PLOT\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(15, 15)\n",
    "\n",
    "    #assign colors based on attributes\n",
    "    weights_e \t= [G[u][v]['weight'] for u,v in G.edges()]\n",
    "\n",
    "    #SAMPLE CMAP FOR COLORS \n",
    "    cmap=plt.cm.get_cmap('Blues')\n",
    "    colors_e \t= [cmap(G[u][v]['weight']*10) for u,v in G.edges()]\n",
    "\n",
    "    #PLOT\n",
    "    nx.draw(\n",
    "    G,\n",
    "    edgecolors=\"black\",\n",
    "    edge_color=colors_e,\n",
    "    node_size=2000,\n",
    "    linewidths=2,\n",
    "    font_size=8,\n",
    "    font_color=\"white\",\n",
    "    font_weight=\"bold\",\n",
    "    width=weights_e,\n",
    "    with_labels=True,\n",
    "    pos=pos,\n",
    "    ax=ax\n",
    "    )\n",
    "    ax.set(title='Dracula')\n",
    "    plt.show()\n",
    "\n",
    "# raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea9dbe9f1ceea520258f7c79d3032f6041e6e0b09a8802a98b59f7606cb93a48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
