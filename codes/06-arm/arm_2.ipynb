{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "pdf-engine: lualatex\n",
        "format:\n",
        "  html:\n",
        "    theme: yeti\n",
        "    toc: true\n",
        "    code-fold: true\n",
        "    code-code-overflow: wrap\n",
        "    toc-title: Contents\n",
        "    error: false\n",
        "    warning: false\n",
        "execute:\n",
        "  echo: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Association Rule Mining (ARM) and Networking\n",
        "\n",
        "## Introduction\n",
        "The data set that will be used for ARM is the Wikipedia data. This data set contains sentences from various Wikipedia pages related to the search terms \"Women's Rights\" and \"Men's Rights.\" Unlike the Wikipedia data used in the SVM section, the Wikipedia data for this section will be composed of observations being individual sentences (instead of multiple sentences as was in the SVM section). For the specific code that achieved these data sets, please refer to the data cleaning section. For further detail about these data sets, please refer to the exploring data section.\n",
        "\n",
        "## Theory\n",
        "Association Rule Mining (ARM) is an unsupervised machine learning algorithm that attempts to discover statistically significant relations between variables in a data set. As the name suggests, ARM is a rule-based algorithm that forms a set of if-then rules to predict the occurrence of an item based on occurrences of other items.\n",
        "\n",
        "We have a couple of metrics that can be changed to find a decent ARM rule set. First, Support gauges the importance of an item set, where 1 is very important and 0 is irrelevant. Second, Confidence gauges the strength (or statistical significance) of a rule, where 1 is a strong rule and 0 is a rule that never occurs. Third, Lift gauges the association between item sets, where 1 is no association, < 1 represents unlikely association, and > 1 represents likely association. Lastly, Support x Confidence (which is the product of Support and Confidence) is another useful metric to ensure that a rule has a Support and Confidence; this would suggest that a rule occurs frequently and is \"strong.\" Ideally, we want our Support close to 1, Confidence close to 1, and Lift > 1.\n",
        "\n",
        "## Methods\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from apyori import apriori\n",
        "import networkx as nx \n",
        "\n",
        "import nltk\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=pd.read_csv('./data/00-raw-data/wiki-crawl-results-arm.csv')\n",
        "\n",
        "corpus = df['text'].to_list()\n",
        "\n",
        "text = '. '.join(corpus) # convert sentences into one long string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter Data\n",
        "\n",
        "Since we only want meaningful words to be in our ARM graph, we filter out some words that aren't relevant and aren't covered by nltk's stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#INITIALIZE\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "add=['000', 'doi' '19', '10', '1960s', '1970s', '1975', '1979', '978', 'isbn', '19th', '20th', 'would', 'also', 'access', 'year', 'without', 'well', 'new', 'york', 'one', 'century', 'since', 'first', 'day', 'time', 'united', 'formed', 'including', 'many', 'states', 'often', 'passed', 'must', 'part', 'made', 'based', 'week', 'organiation', 'form', 'state', 'american', 'world', 'nation', 'country', 'international']\n",
        "for sp in add: \n",
        "    stopwords.append(sp)\n",
        "\n",
        "    tmp = ''\n",
        "printable = set(string.printable)\n",
        "for char in text:\n",
        "    if char in printable:\n",
        "        tmp = tmp + char\n",
        "text = tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize and Lemmatize\n",
        "\n",
        "We tokenize the sentences and lemmatize the words into a suitable format for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NUMBER OF SENTENCES FOUND: 6156\n",
            "VOCAB LENGTH: 12378\n"
          ]
        }
      ],
      "source": [
        "#BREAK INTO CHUNKS (SENTENCES OR OTHERWISE)\n",
        "sentences=nltk.tokenize.sent_tokenize(text)  #SENTENCES\n",
        "\n",
        "print(\"NUMBER OF SENTENCES FOUND:\",len(sentences)); #print(sentences)\n",
        "\n",
        "#CLEAN AND LEMMATIZE\n",
        "keep='0123456789abcdefghijklmnopqrstuvwxy';\n",
        "\n",
        "new_sentences=[]; vocabulary=[]\n",
        "for sentence in sentences:\n",
        "    new_sentence=''\n",
        "\n",
        "    # REBUILD LEMMATIZED SENTENCE\n",
        "    for word in sentence.split():\n",
        "        \n",
        "        #ONLY KEEP CHAR IN \"keep\"\n",
        "        tmp2=''\n",
        "        for char in word: \n",
        "            if(char in keep): \n",
        "                tmp2=tmp2+char\n",
        "            else:\n",
        "                tmp2=tmp2+' '\n",
        "        word=tmp2\n",
        "\n",
        "        #-----------------------\n",
        "        # INSERT CODE TO LEMMATIZE THE WORDS\n",
        "        #-----------------------\n",
        "        new_word = lemmatizer.lemmatize(word)\n",
        "\n",
        "        #REMOVE WHITE SPACES\n",
        "        new_word=new_word.replace(' ', '')\n",
        "\n",
        "        #BUILD NEW SENTENCE BACK UP\n",
        "        if( new_word not in stopwords):\n",
        "            if(new_sentence==''):\n",
        "                new_sentence=new_word\n",
        "            else:\n",
        "                new_sentence=new_sentence+','+new_word\n",
        "            if(new_word not in vocabulary): vocabulary.append(new_word)\n",
        "\n",
        "    #SAVE (LIST OF LISTS)\t\t\n",
        "    new_sentences.append(new_sentence.split(\",\"))\n",
        "print(\"VOCAB LENGTH:\",len(vocabulary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RE-FORMAT THE APRIORI OUTPUT INTO A PANDAS DATA-FRAME WITH COLUMNS \"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"\n",
        "def reformat_results(results):\n",
        "\n",
        "    #CLEAN-UP RESULTS \n",
        "    keep=[]\n",
        "    for i in range(0,len(results)):\n",
        "        # print(\"=====================================\")\n",
        "        # print(results[i])\n",
        "        # print(len(list(results[i])))\n",
        "        for j in range(0,len(list(results[i]))):\n",
        "            # print(results)\n",
        "            if(j>1):\n",
        "                for k in range(0,len(list(results[i][j]))):\n",
        "                    if(len(results[i][j][k][0])!=0):\n",
        "                        #print(len(results[i][j][k][0]),results[i][j][k][0])\n",
        "                        rhs=list(results[i][j][k][0])\n",
        "                        lhs=list(results[i][j][k][1])\n",
        "                        conf=float(results[i][j][k][2])\n",
        "                        lift=float(results[i][j][k][3])\n",
        "                        keep.append([rhs,lhs,supp,conf,supp*conf,lift])\n",
        "                        # keep.append()\n",
        "            if(j==1):\n",
        "                supp=results[i][j]\n",
        "\n",
        "    return pd.DataFrame(keep, columns =[\"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"])\n",
        "\n",
        "# CONVERT DF TO GRAPH ITEMS\n",
        "def convert_to_network(df):\n",
        "    print(df)\n",
        "\n",
        "    #BUILD GRAPH\n",
        "    G = nx.DiGraph()  # DIRECTED\n",
        "    for row in df.iterrows():\n",
        "        # for column in df.columns:\n",
        "        lhs=\"_\".join(row[1][0])\n",
        "        rhs=\"_\".join(row[1][1])\n",
        "        conf=row[1][3]; #print(conf)\n",
        "        if(lhs not in G.nodes): \n",
        "            G.add_node(lhs)\n",
        "        if(rhs not in G.nodes): \n",
        "            G.add_node(rhs)\n",
        "\n",
        "        edge=(lhs,rhs)\n",
        "        if edge not in G.edges:\n",
        "            G.add_edge(lhs, rhs, weight=conf)\n",
        "\n",
        "    # print(G.nodes)\n",
        "    # print(G.edges)\n",
        "    return G\n",
        "\n",
        "# PLOT GRAPH\n",
        "def plot_network(G):\n",
        "    #SPECIFIY X-Y POSITIONS FOR PLOTTING\n",
        "    pos=nx.random_layout(G)\n",
        "\n",
        "    #GENERATE PLOT\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches(15, 15)\n",
        "\n",
        "    #assign colors based on attributes\n",
        "    weights_e \t= [G[u][v]['weight'] for u,v in G.edges()]\n",
        "\n",
        "    #SAMPLE CMAP FOR COLORS \n",
        "    cmap=plt.cm.get_cmap('Blues')\n",
        "    colors_e \t= [cmap(G[u][v]['weight']*10) for u,v in G.edges()]\n",
        "\n",
        "    #PLOT\n",
        "    nx.draw(\n",
        "    G,\n",
        "    edgecolors=\"black\",\n",
        "    edge_color=colors_e,\n",
        "    node_size=4000,\n",
        "    linewidths=2,\n",
        "    font_size=8,\n",
        "    font_color=\"white\",\n",
        "    font_weight=\"bold\",\n",
        "    width=weights_e,\n",
        "    with_labels=True,\n",
        "    pos=pos,\n",
        "    ax=ax\n",
        "    )\n",
        "    ax.set(title='WikiCrawl Word Association Graph')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train ARM Models and Plot Directed Graphs\n",
        "\n",
        "While there isn't a specific way of finding the optimal hyperparameter values, we can try varying support and confidence levels and decide based on the directed graph. We train multiple ARM models with increasing levels of support and confidence. We then plot a directed graph for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transactions:              0           1            2            3            4    \\\n",
            "0     employment       right        woman      include          non   \n",
            "1          right       woman          men        equal          pay   \n",
            "2         leslie         wah        leung        chung         1917   \n",
            "3            job      status        woman      changed    permanent   \n",
            "4           even        lost          job         None         None   \n",
            "...          ...         ...          ...          ...          ...   \n",
            "6151     victory  observance  celebration       soviet        union   \n",
            "6152         may       child     maldives  confederate     memorial   \n",
            "6153         may          11     national   technology        india   \n",
            "6154        iumo      taisha        japan     national  unification   \n",
            "6155         end        june           15         army     slovenia   \n",
            "\n",
            "                 5          6          7             8             9    ...  \\\n",
            "0     discriminatory      woman        job         equal           pay  ...   \n",
            "1              equal    benefit      equal          work        openly  ...   \n",
            "2               2009  president       hong          kong       chinese  ...   \n",
            "3           employee  temporary   employee       married          thus  ...   \n",
            "4               None       None       None          None          None  ...   \n",
            "...              ...        ...        ...           ...           ...  ...   \n",
            "6151         victory        nai    germany        soviet         union  ...   \n",
            "6152           north   carolina      south      carolina  constitution  ...   \n",
            "6153       statehood  minnesota    vietnam         human         right  ...   \n",
            "6154         liberia        may         15     beginning      tourette  ...   \n",
            "6155     constituent   assembly  lithuania  independence      paraguay  ...   \n",
            "\n",
            "        189     190   191   192   193   194   195   196   197   198  \n",
            "0      None    None  None  None  None  None  None  None  None  None  \n",
            "1      None    None  None  None  None  None  None  None  None  None  \n",
            "2      None    None  None  None  None  None  None  None  None  None  \n",
            "3      None    None  None  None  None  None  None  None  None  None  \n",
            "4      None    None  None  None  None  None  None  None  None  None  \n",
            "...     ...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
            "6151   None    None  None  None  None  None  None  None  None  None  \n",
            "6152   None    None  None  None  None  None  None  None  None  None  \n",
            "6153   None    None  None  None  None  None  None  None  None  None  \n",
            "6154   None    None  None  None  None  None  None  None  None  None  \n",
            "6155  saint  helena  1502  None  None  None  None  None  None  None  \n",
            "\n",
            "[6156 rows x 199 columns]\n",
            "            rhs         lhs      supp      conf  supp x conf      lift\n",
            "0         [100]     [woman]  0.002112  0.481481     0.001017  1.190361\n",
            "1       [woman]       [100]  0.002112  0.005221     0.000011  1.190361\n",
            "2          [11]     [woman]  0.002599  0.516129     0.001341  1.276020\n",
            "3       [woman]        [11]  0.002599  0.006426     0.000017  1.276020\n",
            "4          [12]  [abortion]  0.002112  0.351351     0.000742  4.774655\n",
            "...         ...         ...       ...       ...          ...       ...\n",
            "4499  [written]     [woman]  0.002112  0.520000     0.001098  1.285590\n",
            "4500    [woman]     [wrote]  0.003899  0.009639     0.000038  1.348521\n",
            "4501    [wrote]     [woman]  0.003899  0.545455     0.002127  1.348521\n",
            "4502    [woman]     [young]  0.006335  0.015663     0.000099  1.377418\n",
            "4503    [young]     [woman]  0.006335  0.557143     0.003530  1.377418\n",
            "\n",
            "[4504 rows x 6 columns]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\PersonalFolder\\SeniorFall\\anly-501-project-5cminsuhlim\\codes\\06-arm\\arm_2.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pd_results\u001b[39m=\u001b[39mreformat_results(results)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m G\u001b[39m=\u001b[39mconvert_to_network(pd_results)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plot_network(G)\n",
            "\u001b[1;32md:\\PersonalFolder\\SeniorFall\\anly-501-project-5cminsuhlim\\codes\\06-arm\\arm_2.ipynb Cell 13\u001b[0m in \u001b[0;36mplot_network\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m colors_e \t\u001b[39m=\u001b[39m [cmap(G[u][v][\u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m) \u001b[39mfor\u001b[39;00m u,v \u001b[39min\u001b[39;00m G\u001b[39m.\u001b[39medges()]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m#PLOT\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m nx\u001b[39m.\u001b[39;49mdraw(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m G,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m edgecolors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mblack\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m edge_color\u001b[39m=\u001b[39;49mcolors_e,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m node_size\u001b[39m=\u001b[39;49m\u001b[39m4000\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m linewidths\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m font_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m font_color\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mwhite\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m font_weight\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbold\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m width\u001b[39m=\u001b[39;49mweights_e,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m with_labels\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m pos\u001b[39m=\u001b[39;49mpos,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m ax\u001b[39m=\u001b[39;49max\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m ax\u001b[39m.\u001b[39mset(title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWikiCrawl Word Association Graph\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PersonalFolder/SeniorFall/anly-501-project-5cminsuhlim/codes/06-arm/arm_2.ipynb#X15sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:120\u001b[0m, in \u001b[0;36mdraw\u001b[1;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mwith_labels\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwds:\n\u001b[0;32m    118\u001b[0m     kwds[\u001b[39m\"\u001b[39m\u001b[39mwith_labels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwds\n\u001b[1;32m--> 120\u001b[0m draw_networkx(G, pos\u001b[39m=\u001b[39mpos, ax\u001b[39m=\u001b[39max, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    121\u001b[0m ax\u001b[39m.\u001b[39mset_axis_off()\n\u001b[0;32m    122\u001b[0m plt\u001b[39m.\u001b[39mdraw_if_interactive()\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:334\u001b[0m, in \u001b[0;36mdraw_networkx\u001b[1;34m(G, pos, arrows, with_labels, **kwds)\u001b[0m\n\u001b[0;32m    331\u001b[0m     pos \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mdrawing\u001b[39m.\u001b[39mspring_layout(G)  \u001b[39m# default to spring layout\u001b[39;00m\n\u001b[0;32m    333\u001b[0m draw_networkx_nodes(G, pos, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnode_kwds)\n\u001b[1;32m--> 334\u001b[0m draw_networkx_edges(G, pos, arrows\u001b[39m=\u001b[39marrows, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39medge_kwds)\n\u001b[0;32m    335\u001b[0m \u001b[39mif\u001b[39;00m with_labels:\n\u001b[0;32m    336\u001b[0m     draw_networkx_labels(G, pos, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlabel_kwds)\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:889\u001b[0m, in \u001b[0;36mdraw_networkx_edges\u001b[1;34m(G, pos, edgelist, width, edge_color, style, alpha, arrowstyle, arrowsize, edge_cmap, edge_vmin, edge_vmax, ax, arrows, label, node_size, nodelist, node_shape, connectionstyle, min_source_margin, min_target_margin)\u001b[0m\n\u001b[0;32m    887\u001b[0m         _draw_networkx_edges_fancy_arrow_patch()\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     edge_viz_obj \u001b[39m=\u001b[39m _draw_networkx_edges_fancy_arrow_patch()\n\u001b[0;32m    891\u001b[0m \u001b[39m# update view after drawing\u001b[39;00m\n\u001b[0;32m    892\u001b[0m padx, pady \u001b[39m=\u001b[39m \u001b[39m0.05\u001b[39m \u001b[39m*\u001b[39m w, \u001b[39m0.05\u001b[39m \u001b[39m*\u001b[39m h\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:867\u001b[0m, in \u001b[0;36mdraw_networkx_edges.<locals>._draw_networkx_edges_fancy_arrow_patch\u001b[1;34m()\u001b[0m\n\u001b[0;32m    852\u001b[0m     arrow \u001b[39m=\u001b[39m mpl\u001b[39m.\u001b[39mpatches\u001b[39m.\u001b[39mFancyArrowPatch(\n\u001b[0;32m    853\u001b[0m         (x1, y1),\n\u001b[0;32m    854\u001b[0m         (x2, y2),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    863\u001b[0m         zorder\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    864\u001b[0m     )  \u001b[39m# arrows go behind nodes\u001b[39;00m\n\u001b[0;32m    866\u001b[0m     arrow_collection\u001b[39m.\u001b[39mappend(arrow)\n\u001b[1;32m--> 867\u001b[0m     ax\u001b[39m.\u001b[39;49madd_patch(arrow)\n\u001b[0;32m    869\u001b[0m \u001b[39mreturn\u001b[39;00m arrow_collection\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\axes\\_base.py:2363\u001b[0m, in \u001b[0;36m_AxesBase.add_patch\u001b[1;34m(self, p)\u001b[0m\n\u001b[0;32m   2361\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2362\u001b[0m     p\u001b[39m.\u001b[39mset_clip_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch)\n\u001b[1;32m-> 2363\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_patch_limits(p)\n\u001b[0;32m   2364\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_children\u001b[39m.\u001b[39mappend(p)\n\u001b[0;32m   2365\u001b[0m p\u001b[39m.\u001b[39m_remove_method \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_children\u001b[39m.\u001b[39mremove\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\axes\\_base.py:2381\u001b[0m, in \u001b[0;36m_AxesBase._update_patch_limits\u001b[1;34m(self, patch)\u001b[0m\n\u001b[0;32m   2378\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(patch, mpatches\u001b[39m.\u001b[39mRectangle) \u001b[39mand\u001b[39;00m\n\u001b[0;32m   2379\u001b[0m         ((\u001b[39mnot\u001b[39;00m patch\u001b[39m.\u001b[39mget_width()) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m patch\u001b[39m.\u001b[39mget_height()))):\n\u001b[0;32m   2380\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m-> 2381\u001b[0m p \u001b[39m=\u001b[39m patch\u001b[39m.\u001b[39;49mget_path()\n\u001b[0;32m   2382\u001b[0m vertices \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mvertices \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mcodes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m p\u001b[39m.\u001b[39mvertices[np\u001b[39m.\u001b[39misin(\n\u001b[0;32m   2383\u001b[0m     p\u001b[39m.\u001b[39mcodes, (mpath\u001b[39m.\u001b[39mPath\u001b[39m.\u001b[39mCLOSEPOLY, mpath\u001b[39m.\u001b[39mPath\u001b[39m.\u001b[39mSTOP), invert\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)]\n\u001b[0;32m   2384\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vertices\u001b[39m.\u001b[39msize:\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\patches.py:4446\u001b[0m, in \u001b[0;36mFancyArrowPatch.get_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4443\u001b[0m \u001b[39m\"\"\"Return the path of the arrow in the data coordinates.\"\"\"\u001b[39;00m\n\u001b[0;32m   4444\u001b[0m \u001b[39m# The path is generated in display coordinates, then converted back to\u001b[39;00m\n\u001b[0;32m   4445\u001b[0m \u001b[39m# data coordinates.\u001b[39;00m\n\u001b[1;32m-> 4446\u001b[0m _path, fillable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_path_in_displaycoord()\n\u001b[0;32m   4447\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39miterable(fillable):\n\u001b[0;32m   4448\u001b[0m     _path \u001b[39m=\u001b[39m Path\u001b[39m.\u001b[39mmake_compound_path(\u001b[39m*\u001b[39m_path)\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\patches.py:4459\u001b[0m, in \u001b[0;36mFancyArrowPatch._get_path_in_displaycoord\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4457\u001b[0m     posB \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_xy_units(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_posA_posB[\u001b[39m1\u001b[39m])\n\u001b[0;32m   4458\u001b[0m     (posA, posB) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_transform()\u001b[39m.\u001b[39mtransform((posA, posB))\n\u001b[1;32m-> 4459\u001b[0m     _path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_connectionstyle()(posA, posB,\n\u001b[0;32m   4460\u001b[0m                                        patchA\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatchA,\n\u001b[0;32m   4461\u001b[0m                                        patchB\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatchB,\n\u001b[0;32m   4462\u001b[0m                                        shrinkA\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinkA \u001b[39m*\u001b[39;49m dpi_cor,\n\u001b[0;32m   4463\u001b[0m                                        shrinkB\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinkB \u001b[39m*\u001b[39;49m dpi_cor\n\u001b[0;32m   4464\u001b[0m                                        )\n\u001b[0;32m   4465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4466\u001b[0m     _path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_transform()\u001b[39m.\u001b[39mtransform_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path_original)\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:794\u001b[0m, in \u001b[0;36mdraw_networkx_edges.<locals>._draw_networkx_edges_fancy_arrow_patch.<locals>._connectionstyle\u001b[1;34m(posA, posB, *args, **kwargs)\u001b[0m\n\u001b[0;32m    791\u001b[0m     ret \u001b[39m=\u001b[39m mpl\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mPath(ax\u001b[39m.\u001b[39mtransData\u001b[39m.\u001b[39mtransform(path), [\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m])\n\u001b[0;32m    792\u001b[0m \u001b[39m# if not, fall back to the user specified behavior\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 794\u001b[0m     ret \u001b[39m=\u001b[39m base_connection_style(posA, posB, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    796\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\patches.py:2777\u001b[0m, in \u001b[0;36mConnectionStyle._Base.__call__\u001b[1;34m(self, posA, posB, shrinkA, shrinkB, patchA, patchB)\u001b[0m\n\u001b[0;32m   2775\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnect(posA, posB)\n\u001b[0;32m   2776\u001b[0m clipped_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clip(path, patchA, patchB)\n\u001b[1;32m-> 2777\u001b[0m shrunk_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shrink(clipped_path, shrinkA, shrinkB)\n\u001b[0;32m   2778\u001b[0m \u001b[39mreturn\u001b[39;00m shrunk_path\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\patches.py:2758\u001b[0m, in \u001b[0;36mConnectionStyle._Base._shrink\u001b[1;34m(self, path, shrinkA, shrinkB)\u001b[0m\n\u001b[0;32m   2756\u001b[0m insideA \u001b[39m=\u001b[39m inside_circle(\u001b[39m*\u001b[39mpath\u001b[39m.\u001b[39mvertices[\u001b[39m0\u001b[39m], shrinkA)\n\u001b[0;32m   2757\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2758\u001b[0m     left, path \u001b[39m=\u001b[39m split_path_inout(path, insideA)\n\u001b[0;32m   2759\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2760\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\bezier.py:371\u001b[0m, in \u001b[0;36msplit_path_inout\u001b[1;34m(path, inside, tolerance, reorder_inout)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe path does not intersect with the patch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    370\u001b[0m bp \u001b[39m=\u001b[39m bezier_path\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m--> 371\u001b[0m left, right \u001b[39m=\u001b[39m split_bezier_intersecting_with_closedpath(\n\u001b[0;32m    372\u001b[0m     bp, inside, tolerance)\n\u001b[0;32m    373\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(left) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    374\u001b[0m     codes_left \u001b[39m=\u001b[39m [Path\u001b[39m.\u001b[39mLINETO]\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\bezier.py:334\u001b[0m, in \u001b[0;36msplit_bezier_intersecting_with_closedpath\u001b[1;34m(bezier, inside_closedpath, tolerance)\u001b[0m\n\u001b[0;32m    331\u001b[0m bz \u001b[39m=\u001b[39m BezierSegment(bezier)\n\u001b[0;32m    332\u001b[0m bezier_point_at_t \u001b[39m=\u001b[39m bz\u001b[39m.\u001b[39mpoint_at_t\n\u001b[1;32m--> 334\u001b[0m t0, t1 \u001b[39m=\u001b[39m find_bezier_t_intersecting_with_closedpath(\n\u001b[0;32m    335\u001b[0m     bezier_point_at_t, inside_closedpath, tolerance\u001b[39m=\u001b[39;49mtolerance)\n\u001b[0;32m    337\u001b[0m _left, _right \u001b[39m=\u001b[39m split_de_casteljau(bezier, (t0 \u001b[39m+\u001b[39m t1) \u001b[39m/\u001b[39m \u001b[39m2.\u001b[39m)\n\u001b[0;32m    338\u001b[0m \u001b[39mreturn\u001b[39;00m _left, _right\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\bezier.py:169\u001b[0m, in \u001b[0;36mfind_bezier_t_intersecting_with_closedpath\u001b[1;34m(bezier_point_at_t, inside_closedpath, t0, t1, tolerance)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39m# calculate the middle point\u001b[39;00m\n\u001b[0;32m    168\u001b[0m middle_t \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (t0 \u001b[39m+\u001b[39m t1)\n\u001b[1;32m--> 169\u001b[0m middle \u001b[39m=\u001b[39m bezier_point_at_t(middle_t)\n\u001b[0;32m    170\u001b[0m middle_inside \u001b[39m=\u001b[39m inside_closedpath(middle)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m start_inside \u001b[39m^\u001b[39m middle_inside:\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\bezier.py:223\u001b[0m, in \u001b[0;36mBezierSegment.point_at_t\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpoint_at_t\u001b[39m(\u001b[39mself\u001b[39m, t):\n\u001b[0;32m    220\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m    Evaluate the curve at a single point, returning a tuple of *d* floats.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39;49m(t))\n",
            "File \u001b[1;32mc:\\Users\\Eric\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\matplotlib\\bezier.py:217\u001b[0m, in \u001b[0;36mBezierSegment.__call__\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39mEvaluate the Bezier curve at point(s) t in [0, 1].\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39m    Value of the curve for each point in *t*.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    215\u001b[0m t \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(t)\n\u001b[0;32m    216\u001b[0m \u001b[39mreturn\u001b[39;00m (np\u001b[39m.\u001b[39mpower\u001b[39m.\u001b[39mouter(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m t, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_orders[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m--> 217\u001b[0m         \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mpower\u001b[39m.\u001b[39;49mouter(t, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orders)) \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_px\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"Transactions:\",pd.DataFrame(new_sentences))\n",
        "\n",
        "supp_confs = [[0.002, 0.002],[0.02,0.02],[0.2,0.2]]\n",
        "\n",
        "for s, c in supp_confs:\n",
        "    results = list(apriori(new_sentences, min_support=s, min_lift = 1, min_confidence=c, min_length=1, max_length=2))\n",
        "    pd_results=reformat_results(results)\n",
        "    G=convert_to_network(pd_results)\n",
        "    plot_network(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "\n",
        "As we can see, when the support and confidence values are smaller, the association rules created are more lenient, resulting in a directed graph that is far too populated to extract anything meaningful. As the support and confidence increases, the word association graph becomes more and more sparse, keeping only words that meet the minimum support and confidence threshholds, making word associations more decipherable, until the threshholds become too strict and no word association rules can be made.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Interestingly, the meaningful word association graph for the Wikipedia searches appear to heavily relate to women's issues. We can see word relations such as equal => right, woman => equality, and woman => violence. Another notable finding is that the word male only appears with woman. Despite searching for an equal number of Wikipedia pages for the keywords \"Women's Rights\" and \"Men's Rights,\" it appears that significant word associations can only be found for the former keyword, which may suggest that topics relating to Women's Rights are much more interconnected or have more awareness online."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ANLY501')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "eadb65a309ebe0989826d71328347ef52d129ca1bd9af521e4d4f3d539f018a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
