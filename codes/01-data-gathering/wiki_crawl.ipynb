{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import nltk\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS \n",
    "label_list=[\"Women's rights\", \"Men's rights\"]\n",
    "\n",
    "max_num_pages=30\n",
    "sentence_per_chunk=10\n",
    "min_sentence_length=20\n",
    "\n",
    "# GET STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# INITALIZE STEMMER+LEMITZIZER+SIA\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define text cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "\t# #FILTER OUT UNWANTED CHAR\n",
    "\tnew_text=\"\"\n",
    "\t# keep=string.printable\n",
    "\tkeep=\" abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "\tfor character in text:\n",
    "\t\tif character.lower() in keep:\n",
    "\t\t\tnew_text+=character.lower()\n",
    "\t\telse: \n",
    "\t\t\tnew_text+=\" \"\n",
    "\ttext=new_text\n",
    "\t# print(text)\n",
    "\n",
    "\t# #FILTER OUT UNWANTED WORDS\n",
    "\tnew_text=\"\"\n",
    "\tfor word in nltk.tokenize.word_tokenize(text):\n",
    "\t\tif word not in stop_words:\n",
    "\t\t\t#lemmatize \n",
    "\t\t\ttmp=lemmatizer.lemmatize(word)\n",
    "\t\t\t# tmp=stemmer.stem(tmp)\n",
    "\n",
    "\t\t\t# update word if there is a change\n",
    "\t\t\t# if(tmp!=word): print(tmp,word)\n",
    "\t\t\t\n",
    "\t\t\tword=tmp\n",
    "\t\t\tif len(word)>1:\n",
    "\t\t\t\tif word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n",
    "\t\t\t\t\t#remove the last space\n",
    "\t\t\t\t\tnew_text=new_text[0:-1]+word+\" \"\n",
    "\t\t\t\telse: #add a space\n",
    "\t\t\t\t\tnew_text+=word.lower()+\" \"\n",
    "\ttext=new_text.strip()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Wiki crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE \n",
    "corpus=[]  # list of strings (input variables X)\n",
    "targets=[] # list of targets (labels or response variables Y)\n",
    "\n",
    "#--------------------------\n",
    "# LOOP OVER TOPICS \n",
    "#--------------------------\n",
    "for label in label_list:\n",
    "\n",
    "\t#SEARCH FOR RELEVANT PAGES \n",
    "\ttitles=wikipedia.search(label,results=max_num_pages)\n",
    "\tprint(\"Pages for label =\",label,\":\",titles)\n",
    "\n",
    "\t#LOOP OVER WIKI-PAGES\n",
    "\tfor title in titles:\n",
    "\t\ttry:\n",
    "\t\t\tprint(\"\t\",title)\n",
    "\t\t\twiki_page = wikipedia.page(title, auto_suggest=True)\n",
    "\n",
    "\t\t\t# LOOP OVER SECTIONS IN ARTICLE AND GET PAGE TEXT\n",
    "\t\t\tfor section in wiki_page.sections:\n",
    "\t\t\t\ttext=wiki_page.section(section); #print(text)\n",
    "\n",
    "\t\t\t\t#BREAK IN TO SENTENCES \n",
    "\t\t\t\tsentences=nltk.tokenize.sent_tokenize(text)\n",
    "\t\t\t\tcounter=0\n",
    "\t\t\t\ttext_chunk=''\n",
    "\n",
    "\t\t\t\t#LOOP OVER SENTENCES \n",
    "\t\t\t\tfor sentence in sentences:\n",
    "\t\t\t\t\tif len(sentence)>min_sentence_length:\n",
    "\t\t\t\t\t\tif(counter%sentence_per_chunk==0 and counter!=0):\n",
    "\t\t\t\t\t\t\t# PROCESS COMPLETED CHUNK \n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t# CLEAN STRING\n",
    "\t\t\t\t\t\t\ttext_chunk=clean_string(text_chunk)\n",
    "\n",
    "\t\t\t\t\t\t\t# REMOVE LABEL IF IN STRING (MAKES IT TOO EASY)\n",
    "\t\t\t\t\t\t\ttext_chunk=text_chunk.replace(label,\"\")\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t# REMOVE ANY DOUBLE SPACES\n",
    "\t\t\t\t\t\t\ttext_chunk=' '.join(text_chunk.split()).strip()\n",
    "\n",
    "\t\t\t\t\t\t\t#UPDATE CORPUS \n",
    "\t\t\t\t\t\t\tcorpus.append(text_chunk)\n",
    "\n",
    "\t\t\t\t\t\t\t#UPDATE TARGETS\n",
    "\t\t\t\t\t\t\tscore=sia.polarity_scores(text_chunk)\n",
    "\t\t\t\t\t\t\ttarget=[label,score['compound']]\n",
    "\t\t\t\t\t\t\ttargets.append(target)\n",
    "\n",
    "\t\t\t\t\t\t\t#print(\"TEXT\\n\",text_chunk,target)\n",
    "\n",
    "\t\t\t\t\t\t\t# RESET CHUNK FOR NEXT ITERATION \n",
    "\t\t\t\t\t\t\ttext_chunk=sentence\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\ttext_chunk+=sentence\n",
    "\t\t\t\t\t\t#print(\"--------\\n\", sentence)\n",
    "\t\t\t\t\t\tcounter+=1\n",
    "\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"WARNING: SOMETHING WENT WRONG:\", title);  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECKS AND PRINT TO FILE \n",
    "print(\"number of text chunks = \",len(corpus))\n",
    "print(\"number of targets = \",len(targets))\n",
    "\n",
    "tmp=[]\n",
    "for i in range(0,len(corpus)):\n",
    "    tmp.append([corpus[i],targets[i][0],targets[i][1]])\n",
    "df=pd.DataFrame(tmp)\n",
    "df=df.rename(columns={0: \"text\", 1: \"label\", 2: \"sentiment\"})\n",
    "print(df)\n",
    "df.to_csv('wiki-crawl-results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eadb65a309ebe0989826d71328347ef52d129ca1bd9af521e4d4f3d539f018a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
